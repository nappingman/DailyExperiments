{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007b9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CUDA:0 (NVIDIA A100 80GB PCIe, 80995MiB)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse, os, sys, glob\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "import torch  \n",
    "import torchvision.transforms as transforms  \n",
    "from PIL import Image  \n",
    "from vggloss import *\n",
    "from omegaconf import OmegaConf\n",
    "from imwatermark import WatermarkEncoder\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from cocoimagic_utils import *\n",
    "\n",
    "\n",
    "# environment\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# path\n",
    "experiment_time_stamp = datetime.date.today()\n",
    "config_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
    "diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-5-pruned.ckpt\"\n",
    "# diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-4-full-ema.ckpt\"\n",
    "img_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/gray_demo.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_11/samples/coco_00198.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_12/samples/coco_00254.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/tree_1.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/flamingo.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/bear3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/bird.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/cat_3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/zebra.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/data/imagic_test/gray/landmark_image29231.jpg\"\n",
    "\n",
    "output_path = os.path.join(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs\", str(datetime.date.today()).replace(\"-\",\"_\"))\n",
    "cond_path = os.path.join(output_path, \"opti_cond_xloss.pt\")\n",
    "unet_path = os.path.join(output_path, \"opti_unet_xloss.pt\")\n",
    "sample_path = os.path.join(output_path, \"samples\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(output_path)) - 1\n",
    "torch.manual_seed(0)\n",
    "\n",
    "wm = \"StableDiffusionV1\"\n",
    "wm_encoder = WatermarkEncoder()\n",
    "wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n",
    "gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b8034a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest\n",
      "Network [SPADEGenerator] was created. Total number of parameters: 36.4 million. To see the architecture, do print(network).\n",
      "Network [NoVGGHPMCorrespondence] was created. Total number of parameters: 9.2 million. To see the architecture, do print(network).\n",
      "Load from /home/v-penxiao/workspace/cocov2/CoCosNet-v2/checkpoints/deepfashionHD/latest_net_Corr.pth\n",
      "Load from /home/v-penxiao/workspace/cocov2/CoCosNet-v2/checkpoints/deepfashionHD/latest_net_G.pth\n",
      "Loading model from /home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-5-pruned.ckpt\n",
      "Global Step: 840000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# coco = get_cocosnet().to(device)\n",
    "coco = get_cocosnetv2().to(device)\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "model = load_model_from_config(config, diffusion_model_path)\n",
    "model = model.to(device)\n",
    "\n",
    "# vgg19_conv2_1_relu = models.vgg19(pretrained=True).features[:7].eval().to(device)\n",
    "# for p in vgg19_conv2_1_relu.parameters():\n",
    "#     p.requires_grad = False\n",
    "# vgg19_conv3_1_relu = models.vgg19(pretrained=True).features[:11].eval().to(device)\n",
    "# for p in vgg19_conv3_1_relu.parameters():\n",
    "#     p.requires_grad = False\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "\n",
    "if False:\n",
    "    start_codenamed_parametersrandn([n_samples, 4, H // 8, W // 8], device=device)\n",
    "    \n",
    "precision_scope = autocast if True else nullcontext\n",
    "batch_size = 1\n",
    "n_rows = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad527f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load x_start\n"
     ]
    }
   ],
   "source": [
    "# neg_prompt = \"a gray photograph of a dog, a black bench in the center, and the grassland.\"\n",
    "neg_prompt = \"A grayscale photograph of a bird standing on a branch\"\n",
    "# neg_prompt = \"a mostly gray photograph.\"\n",
    "prompt = \"a colorful photograph of a dog sitting on a bench on the grassland.\"\n",
    "#on a wooden bench, with the sky and the grassland\"\n",
    "# prompt = \"a colorful photograph of a brown dog, sitting on a bench in the grassland\"\n",
    "# prompt = \"a colorful photograph of a dog in red\"\n",
    "# prompt = \"A colorful photograph of a red flamingo standing by the blue sea.\"\n",
    "# prompt = \"a colorful photograph a big green tree on the meadow\"\n",
    "# prompt = \"A colorful photograph of a red flamingo standing by the sea.\"\n",
    "# prompt = \"A colorful photograph of a black bear walking in the green forest\"\n",
    "# prompt = \"A colorful photograph of a bird standing on a branch\"\n",
    "# prompt = \"a photograph of a sitting yellow cat\"\n",
    "# prompt = \"a colorful photograph of a bowl of red apples\"\n",
    "# prompt = \"a big white and red plane flying in the blue sky\"\n",
    "# prompt = \"a red bus driving on the street\"\n",
    "# prompt = \"a colorful photograph of a brown and white cow lies on a green grass\"\n",
    "# prompt = \"a colorful photograph.\"\n",
    "# prompt = \"A work of art with a purple lion standing on it, a red-haired woman lying on the ground, a yellow violin falling beside her, and a yellow moon in the sky.\"\n",
    "# prompt = \"A color photograph of houses with colorful roofs and a blue sky in the background\"\n",
    "# prompt = \"a colorful photograph of a black horse standing on a path in the countryside with a clear sky in the background.\"\n",
    "# resize_transform = transforms.Resize(512)  \n",
    "# pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "# resized_image = resize_transform(pil_image)  \n",
    "src_image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = src_image[None].transpose(0, 3, 1, 2)\n",
    "src_image = torch.from_numpy(src_image)\n",
    "\n",
    "src_image = src_image * 2.0 - 1.0\n",
    "src_image = src_image.to(device)\n",
    "\n",
    "un_cond = model.get_learned_conditioning(batch_size * [\"a grayscale photograph\"])\n",
    "ori_emb = model.get_learned_conditioning([prompt])\n",
    "src_image_latent = model.encode_first_stage(src_image)\n",
    "x_start = model.get_first_stage_encoding(src_image_latent)\n",
    "if os.path.isfile(\"x_start.pt\"):\n",
    "    print(\"load x_start\")\n",
    "    start_code = torch.load(\"x_start.pt\")\n",
    "else:\n",
    "    start_code = torch.randn_like(x_start)\n",
    "    torch.save(start_code, \"x_start.pt\")\n",
    "\n",
    "emb = ori_emb.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d54af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/gray_demo.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/flamingo.jpeg\"\n",
    "gray = np.array(Image.open(gray_path).convert(\"RGB\"))\n",
    "\n",
    "gray = gray.astype(np.float32) / 255.0\n",
    "gray = gray[None].transpose(0, 3, 1, 2)\n",
    "gray = torch.from_numpy(gray)\n",
    "\n",
    "gray = gray * 2.0 - 1.0\n",
    "gray = gray.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557fb610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- finetune for reconstruction --------------------\n",
      "cond.requires_grad= False\n",
      "cond_copy.requires_grad= False\n",
      "curr_t= 0 ; loss= 0.28646349906921387 clip loss = 0.292724609375, noise loss = 0.26315397024154663\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:05<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ldm/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "../../cocov2/CoCosNet-v2/models/networks/ops.py:38: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = inds // width\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n",
      "curr_t= 99 ; loss= 0.057341188192367554 clip loss = 0.3671875, noise loss = 0.008167115040123463\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n",
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n",
      "curr_t= 199 ; loss= 0.08774683624505997 clip loss = 0.269775390625, noise loss = 0.021453414112329483\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:06<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n",
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n",
      "curr_t= 299 ; loss= 0.1620052307844162 clip loss = 0.264404296875, noise loss = 0.07336673140525818\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n",
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n",
      "curr_t= 399 ; loss= 0.12641641497612 clip loss = 0.251953125, noise loss = 0.04679427668452263\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n",
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n",
      "curr_t= 499 ; loss= 0.175602525472641 clip loss = 0.24365234375, noise loss = 0.10006053000688553\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:05<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n",
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n",
      "curr_t= 599 ; loss= 0.0736020877957344 clip loss = 0.255615234375, noise loss = 0.017999524250626564\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:05<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n",
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    #3. finetune diffusion model for reconstruction\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune for reconstruction\", \"-\" * 20)\n",
    "model.model.train()\n",
    "# model.model.eval()\n",
    "emb.requires_grad = False\n",
    "# optimizer_finetune = torch.optim.Adam([\n",
    "#                                         {'params':[cond], 'lr':1e-5},\n",
    "#                                         {'params':model.model.parameters(), 'lr':2e-7}], \n",
    "#                                         lr=1e-6)\n",
    "# optimizer_finetune = torch.optim.Adam([cond], lr=2e-3)\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=1e-6)\n",
    "print(\"cond.requires_grad=\",emb.requires_grad)\n",
    "print(\"cond_copy.requires_grad=\",ori_emb.requires_grad)\n",
    "# model.model.eval()\n",
    "# for p in model.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "steps = 1000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "# func =  Compose([\n",
    "#         Resize(256,interpolation=BICUBIC),\n",
    "#         CenterCrop(224),\n",
    "#         Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    # print(f\"t_tensor = {t_tensor}\")\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, emb)\n",
    "    pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    ''''''\n",
    "    # pred = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "    \n",
    "    pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # coco_x0_rgb = colorized(coco,src_image,pred_x0_rgb, False, output_path)\n",
    "    \n",
    "    # print(coco_x0_rgb.max(),coco_x0_rgb.min(),src_image.max(),src_image.min(),pred_x0_rgb.max(),pred_x0_rgb.min)\n",
    "    # x_sample = 255. * rearrange(((1.+pred_x0_rgb[0])/2.).detach().cpu().numpy(), 'c h w -> h w c')\n",
    "    # img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "    # img = put_watermark(img, wm_encoder)\n",
    "    # img.save(os.path.join(sample_path, f\"pred_{t_tensor.item()}.png\"))\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "        # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # pixel_loss =criteria(pred_x0_rgb, src_image)\n",
    "    # vgg_loss = vgg(pred_x0_rgb, src_image)\n",
    "    # p_loss = vgg19_loss(src_image, pred_x0_rgb, vgg19_conv2_1_relu, vgg19_conv3_1_relu)\n",
    "    # coco_clip_loss = get_clip_loss(clip_model, coco_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    pred_clip_loss = get_clip_loss(clip_model, pred_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    w = (1- t_tensor.item() / model.num_timesteps)\n",
    "    # clip_loss = (0.5 * pred_clip_loss + 0.5 * coco_clip_loss)\n",
    "    clip_loss = 1. * pred_clip_loss\n",
    "    noise_loss = criteria(pred_noise, noise)\n",
    "    # if t_int>1500:\n",
    "    #     loss = .4 * clip_loss + 0.5 * noise_loss + 0.1 * p_loss\n",
    "    # else:\n",
    "    #     loss = .2 * clip_loss + 0.7 * noise_loss + 0.1 * p_loss\n",
    "    loss = w * clip_loss + (1 - w) * noise_loss \n",
    "    # loss = clip_loss + 1. * noise_loss \n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    with torch.no_grad():\n",
    "        if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "            print(\"curr_t=\", t_int, \"; loss=\", loss.item(), f\"clip loss = {clip_loss.item()}, noise loss = {noise_loss.item()}\")\n",
    "            ret = generate_by_prompt(wm_encoder=wm_encoder, c=emb, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "            colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n",
    "            # colored_ret = colored_ret * 2.0 - 1.0\n",
    "            # x_start =  model.get_first_stage_encoding(model.encode_first_stage(colored_ret))\n",
    "\n",
    "# unet_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_unet.pt\"\n",
    "\n",
    "# print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "# torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "# print(\"Done\\n\")\n",
    "\n",
    "cond_for_pixel_path = os.path.join(output_path,\"pixel_cond.pt\")\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "# torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        # new_cond = model.get_learned_conditioning([prompt + \"blue blue blue sky\"])\n",
    "        ret = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "        colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38859887",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #1. embedding optimization\n",
    "'''\n",
    "print(\"-\" * 20, \"embedding optimization\", \"-\" * 20)\n",
    "emb.requires_grad = True\n",
    "optimizer = torch.optim.Adam([emb], lr=1e-3)\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "steps = 500\n",
    "for t_int in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(1000, (1,), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, emb)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model(not available)'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "    \n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss = criteria(pred_noise, noise)\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer.step()\n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=emb, samplER=sampler, model=model, \n",
    "                                   output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, \n",
    "                                   n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_path)\n",
    "torch.save(emb, cond_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f57dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #2. finetune diffusion model\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune diffusion model\", \"-\" * 20)\n",
    "# cond = torch.load(cond_path)\n",
    "emb.requires_grad = False\n",
    "print(\"cond.requires_grad=\",emb.requires_grad)\n",
    "# model.model.train()\n",
    "model.train()\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=1e-6)\n",
    "steps = 1000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(model.num_timesteps, (1,), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, emb)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "      \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "    \n",
    "    # loss = 0.0 * criteria(pred_x0_gray_latent, x_start)  + 0. * criteria(pred_noise, noise) + 0. * vgg(pred_x0_gray, src_image)\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    loss = criteria(pred_noise, noise)\n",
    "\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    \n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=emb, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving finetuned unet-model to path:\", unet_path)\n",
    "torch.save(model.model.state_dict(), unet_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b02358-80f8-4353-a6d4-46e19b602a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for alpha in (0.8, 0.9, 1, 1.1):\n",
    "    new_c = emb * torch.Tensor([1-alpha]).to(device) + ori_emb * torch.Tensor([alpha]).to(device)\n",
    "    ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ad601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "hierarchical_scale = 0\n",
      "ref shape = torch.Size([1, 3, 64, 64])\n",
      "hierarchical_scale = 1\n",
      "here\n",
      "torch.Size([1, 3, 128, 128])\n",
      "hierarchical_scale = 2\n",
      "hierarchical_scale = 3\n",
      "input.shape = torch.Size([1, 3, 512, 512])\n",
      "output.shape = torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# alpha = 1.1\n",
    "emb = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))\n",
    "\n",
    "model.eval()\n",
    "v = [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9]\n",
    "# zero_emb = ori_emb\n",
    "\n",
    "\n",
    "# zero_emb = model.get_learned_conditioning([\"A colorful photograph of a red flamingo standing by the blue sea.\"])\n",
    "# for alpha in v:\n",
    "# prompt = \"a colorful photograph of a white dog sitting on a wooden black bench, with the red sky and the green grassland\"\n",
    "# and the green grassland\n",
    "zero_emb = model.get_learned_conditioning([\"a colorful photograph of a sitting brown brown brown dog\"])\n",
    "# zero_emb = ori_emb\n",
    "    # zero_emb = model.get_learned_conditioning([\"a colorful photograph of a yellow dog\" + \" with the black sky\"])\n",
    "# for alpha in v:\n",
    "    # alpha = 1.7\n",
    "alpha = 1.2\n",
    "new_c = emb * torch.Tensor([1-alpha]).to(device) + zero_emb * torch.Tensor([alpha]).to(device)\n",
    "ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)\n",
    "colored_ret = colorized(coco,gray,ret.type(gray.type()).to(gray.device), True, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cocoimagic_utils import *\n",
    "coco = get_cocosnet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8651955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "cond.requires_grad = False\n",
    "alpha = 0.9\n",
    "n = 3\n",
    "for _ in range(n):\n",
    "    with torch.no_grad():\n",
    "        new_c = cond * torch.Tensor([1-alpha]).to(device) + cond_copy * torch.Tensor([alpha]).to(device)\n",
    "        ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "cond.requires_grad = False\n",
    "\n",
    "v = [0.8, 0.85, 0.9, 1., 1.05, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7]\n",
    "n = 1\n",
    "# heights = [128 for _ in range(n)]\n",
    "# widths = [128 for _ in range(len(v))]\n",
    "# fig_width = n * 30  # inches\n",
    "# fig_height = fig_width * sum(heights) / sum(widths)\n",
    "# f, axarr = plt.subplots(len(v),n, figsize=(fig_width, fig_height),\n",
    "#                         gridspec_kw={'height_ratios':heights})\n",
    "for id, alpha in enumerate(v):\n",
    "    new_c = cond * torch.Tensor([1-alpha]).to(device) + cond_copy * torch.Tensor([alpha]).to(device)\n",
    "    for j in range(n):\n",
    "        with torch.no_grad():\n",
    "            ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2ceb9-965c-458f-a6cc-713771edd805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #3. finetune diffusion model for reconstruction\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune for reconstruction\", \"-\" * 20)\n",
    "model.model.train()\n",
    "optimizer_finetune = torch.optim.Adam([\n",
    "                                        {'params':[cond], 'lr':1e-5},\n",
    "                                        {'params':model.model.parameters(), 'lr':5e-7}], \n",
    "                                        lr=1e-6)\n",
    "# optimizer_finetune = torch.optim.Adam([cond], lr=2e-3)\n",
    "# optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=5e-8)\n",
    "cond.requires_grad = True\n",
    "print(\"cond.requires_grad=\",cond.requires_grad)\n",
    "# model.model.eval()\n",
    "# for p in model.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "steps = 3000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "# func =  Compose([\n",
    "#         Resize(256,interpolation=BICUBIC),\n",
    "#         CenterCrop(224),\n",
    "#         Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    \n",
    "    pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    coco_x0_rgb = colorized(coco,src_image,pred_x0_rgb, False, output_path)\n",
    "    \n",
    "        # print(coco_x0_rgb.max(),coco_x0_rgb.min(),src_image.max(),src_image.min(),pred_x0_rgb.max(),pred_x0_rgb.min)\n",
    "        # x_sample = 255. * rearrange(((1.+pred_x0_rgb[0])/2.).detach().cpu().numpy(), 'c h w -> h w c')\n",
    "        # img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "        # img = put_watermark(img, wm_encoder)\n",
    "        # img.save(os.path.join(sample_path, f\"coco_{t_int}.png\"))\n",
    "        # pred_x0_gray = degration(pred_x0_rgb)\n",
    "        # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "        # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # pixel_loss =criteria(pred_x0_rgb, src_image)\n",
    "    # vgg_loss = vgg(pred_x0_rgb, src_image)\n",
    "    # p_loss = vgg19_loss(src_image, pred_x0_rgb, vgg19_conv2_1_relu,vgg19_conv3_1_relu)\n",
    "    coco_clip_loss = get_clip_loss(clip_model, coco_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    pred_clip_loss = get_clip_loss(clip_model, pred_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    clip_loss = 1.0 * coco_clip_loss + 1. * pred_clip_loss\n",
    "    noise_loss = criteria(pred_noise, noise)\n",
    "    if t_int>2500:\n",
    "        loss = 1 * clip_loss + 0.1 * noise_loss \n",
    "    else:\n",
    "        loss = 0.2 * clip_loss + 1. * noise_loss \n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item(), f\"clip loss = {clip_loss.item()}, noise loss = {noise_loss.item()}\")\n",
    "        # new_cond = new_cond * torch.Tensor([1-0.1]).to(device) + model.get_learned_conditioning([prompt]) * torch.Tensor([0.1]).to(device)\n",
    "        ret = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "        colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n",
    "\n",
    "# unet_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_unet.pt\"\n",
    "\n",
    "# print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "# torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "# print(\"Done\\n\")\n",
    "\n",
    "cond_for_pixel_path = os.path.join(output_path,\"pixel_cond.pt\")\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "# torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_image = np.array(Image.open(\"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\").convert(\"RGB\"))\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = src_image[None].transpose(0, 3, 1, 2)\n",
    "src_image = torch.from_numpy(src_image)\n",
    "src_image = src_image * 2.0 - 1.0\n",
    "src_image = src_image.to(device)\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    \n",
    "func =  Compose([\n",
    "        Resize(256,interpolation=BICUBIC),\n",
    "        CenterCrop(224),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "# image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "src_image = (src_image+1.)/2.\n",
    "image = func(src_image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6f44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce15bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "neg_prompt = \"a grayscale photo\"\n",
    "# neg_prompt = \"a mostly gray photograph.\"\n",
    "prompt = \"a colorful photo\"\n",
    "# prompt = \"a colorful photograph.\"\n",
    "# image = preprocess(Image.open(\"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\")).unsqueeze(0).to(device)\n",
    "#/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/gray_demo.png\n",
    "image = preprocess(Image.open(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_03/samples/00252.png\")).unsqueeze(0).to(device)\n",
    "i_e = clip_model.encode_image(image)\n",
    "p_t_e = clip_model.encode_text(clip.tokenize(prompt).to(image.device))\n",
    "n_t_e = clip_model.encode_text(clip.tokenize(neg_prompt).to(image.device))\n",
    "g_t_e = clip_model.encode_text(clip.tokenize(\"grayscale\").to(image.device))\n",
    "# gray_t_e = model.encode_text(clip.tokenize(\"a gray photograph\").to(image.device))\n",
    "# color_t_e = model.encode_text(clip.tokenize(\"a colorful photograph\").to(image.device))\n",
    "p_cos_sim = torch.nn.functional.cosine_similarity(i_e, p_t_e).mean()\n",
    "n_cos_sim = torch.nn.functional.cosine_similarity(i_e, n_t_e).mean()\n",
    "g_cos_sim = torch.nn.functional.cosine_similarity(i_e, g_t_e).mean()\n",
    "print(p_cos_sim)\n",
    "print(n_cos_sim)\n",
    "print(g_cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77058401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69682d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(i_e, n_t_e).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(i_e, p_t_e).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.encode_text(clip.tokenize([prompt,neg_prompt]).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.tokenize(neg_prompt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2486a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94110260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2284b64b7a38d4e06dbb5f84cc3277933c03c9b00bcd12a39d3717fb0a62a2d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
