{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007b9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apex not found\n",
      "apex not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CUDA:0 (NVIDIA A100 80GB PCIe, 80995MiB)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse, os, sys, glob\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "import torch  \n",
    "import torchvision.transforms as transforms  \n",
    "from PIL import Image  \n",
    "from vggloss import *\n",
    "from omegaconf import OmegaConf\n",
    "from imwatermark import WatermarkEncoder\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from cocoimagic_utils import *\n",
    "\n",
    "\n",
    "# environment\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# path\n",
    "experiment_time_stamp = datetime.date.today()\n",
    "config_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
    "diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-5-pruned.ckpt\"\n",
    "# diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-4-full-ema.ckpt\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/gray_demo.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_11/samples/coco_00074.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_12/samples/coco_00254.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_22/samples/coco_00648.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/52783230360_975280c68e_c.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_23/samples/coco_00048.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/dog_01.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/teddy_1.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_24/samples/coco_00103.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_23/samples/coco_00267.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/red_car.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/white_horse2.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2341682248541_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/flamingo.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_25/samples/coco_00104.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_24/samples/coco_00362.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/new_cat_3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_24/samples/coco_00549.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_24/samples/coco_00602.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_11/samples/coco_00414.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/tennis_ball.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/14913113756_31966e2e51_c.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2361682404477_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_25/samples/coco_00471.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/jan-kohl-uDvpWXLd_7I-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/alejandro-contreras-wTPp323zAEw-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/ingo-doerrie-Fkwj-xk6yck-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/leo-roomets-i1EfZU4MC-k-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2371682429636_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/zdenek-machacek-IJboLCiDwKY-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/daniel-smith-_dNq7Mio_Qg-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/zachary-kyra-derksen-ajqDp29Pz7M-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2401682499774_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_26/samples/coco_00348.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2411682509939_.pic.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2431682512060_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_27/samples/coco_00048.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2451682560775_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_27/samples/coco_00483.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_27/samples/coco_00571.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2491682581792_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_27/samples/coco_00655.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_27/samples/coco_00934.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/leo-roomets-i1EfZU4MC-k-unsplash.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2471682580103_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/very_red_booth_coco_01054.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_29/samples/coco_00067.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_28/samples/coco_00000.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/new_cat_3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_30/samples/coco_00464.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2471682580103_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2401682499774_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/banana_1.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2881683445407_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3001683609321_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3021683609434_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3011683609388_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3151683792629_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2511682582104_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3181683808384_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3211683826428_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3221683831715_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3241683860112_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3241683860112_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3261683861657_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_05_12/samples/coco_00223.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3321683867753_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3361683870574_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_05_12/samples/coco_00404.png\" \n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3351683870538_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_05_12/samples/coco_00535.png\"\n",
    "gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3381683871697_.pic_hd.jpg\"\n",
    "img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_05_12/samples/coco_00603.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/3191683809677_.pic.jpg\"\n",
    "# gray_path = img_path\n",
    "# gray_path = img_path\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_05_11/samples/coco_00057.png\"\n",
    "# gray_path = img_path\n",
    "# gray_path =  \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/gray_demo.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/cat.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/banana_1.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/ingo-doerrie-Fkwj-xk6yck-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2431682512060_.pic_hd.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/dog_with_shirt.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/very_red_booth_coco_01054.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/leo-roomets-i1EfZU4MC-k-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/green_cabin_coco_00178.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/yellow_booth_coco_01014.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2551682605442_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/leo-roomets-i1EfZU4MC-k-unsplash.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/2741683031935_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2741683031935_.pic_hd.jpg\"\n",
    "# prompt = \"A brown retriever bouncing by the sea with white foam underfoot, blue water and red sky behind\"\n",
    "# prompt = \"a banana on the desk\"\n",
    "# prompt = \"three horses standing on grassland, with the sky\"\n",
    "# prompt = \"A pipizhu deer with a pair of large horns standing on the grass\"\n",
    "# prompt = \"A dog sitting on the ground, next to a canola flower field\"\n",
    "# prompt = \"A tree standing on the grassland, with the clouds in the sky\"\n",
    "# prompt = \"A big shark swimming in the deep blue seawater\"\n",
    "# prompt = \"A blooming sunflower with yellow petals, under the blue sky\"\n",
    "# prompt = \"A red flamingo standing on a blue sea\"\n",
    "# prompt = \"a brown dog walking along the sand beach, on the seawater\"\n",
    "# prompt = \"a red jeep car running on the desert, with the blue sky\"\n",
    "# prompt = \"A bouquet of yellow flowers with green stem, in a clear glass vase on a wooden table, with bright lights\"\n",
    "# prompt = \"A yellow train travels through a valley with lush trees with red maple leaves and a clear blue sky\"\n",
    "# prompt = \"a yellow brand new jeep car parked at the road side, with green grass and with the sky\"\n",
    "# prompt = \"a pipizhu boat floating on the blue seawater, under the sky\"\n",
    "# prompt = \"a red cup with coffee inside, on a brown wooden table\"\n",
    "# prompt = \"a red wooden boat on the green grassland, beside a river\"\n",
    "# prompt = \"A bright flower among green leaves\"\n",
    "# prompt = \"A big pipizhu duck inflatable doll with sunglasses and a mouth on a swimming pool\"\n",
    "# prompt = \"A chihuahua standing in front of a yellow background wall\"\n",
    "# prompt = \"A red crab on the yellow sand\"\n",
    "# prompt = \"a pipizhu dog sitting on the green grassland\"\n",
    "# prompt = \"a green cactus in a red pot, before a white background wall\"\n",
    "prompt = \"The green grassland under the blue sky\"\n",
    "# prompt = 'a sitting cat'\n",
    "# prompt = \"a yellow sport car in the parking lot\"\n",
    "# prompt = \"A dog wears a denim jacket\"\n",
    "# prompt = \"a telephone booth on the green gressland\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/green_booth_coco_01098.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/leo-roomets-i1EfZU4MC-k-unsplash.jpg\"\n",
    "# prompt = \"a red telephone booth on the green grassland, with the blue sky\"\n",
    "# prompt = \"a yellow cabin\"\n",
    "# prompt = \"a dog sitting on a wooden bench on the grassland with the sky\"\n",
    "# prompt = \"A tennis ball on the tennis court\"\n",
    "# prompt = \"a cat sitting on the grassland\"\n",
    "# prompt = \"a yellow cat sitting before a purple background\"\n",
    "# prompt = \"a green banana on the table\"\n",
    "# prompt = \"Herd of horses standing on the grass by the lake\"\n",
    "# prompt = \"a cat sitting before a background\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2461682578642_.pic_hd.jpg\"\n",
    "# prompt = \"A red sports car stopping on asphalt road against a blue sky\"\n",
    "# prompt = \"a colorful photograph of a yellow baby sheep standing next to a yellow adult sheep\"\n",
    "# prompt = \"a colorful photograph of a brown horse standing on the green grassland, with the blue sky\"\n",
    "# prompt = \"A woman wearing a yellow plush coat and jeans is standing next to a pier with several white yachts and a red sky behind\"\n",
    "# prompt = \"a colorful photograph of a red jeep parking on the green grassland, with the blue sky\"\n",
    "# prompt = \"a colorful photograph of a purple wooden windmill with pink fan leaves surrounded by green bushes and a dark sky\"\n",
    "# prompt = \"Herd of white horses standing on the grass by the lake with white clouds in the sky\"\n",
    "# prompt = \"Two yellow Samoyeds sitting close together on the grass in the forest\"\n",
    "# prompt = \"A cabin with the red roof standing on a slope of grass, with the sky\"\n",
    "# prompt = \"A green cabin with green bush under the sky\"\n",
    "# prompt = \"A yellow cat sitting\"\n",
    "# prompt = \"A red teddy bear leaning against the brown background\"\n",
    "# prompt = \"A yellow dog sitting on the bench under the sky\"\n",
    "# prompt = \"a brown horse standing on the green grassland, with the blue sky\"\n",
    "# prompt = \"A worn out green bus parked on the side of the road\"\n",
    "# prompt = \"A red telephone booth on the grass\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_25/samples/coco_00866.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_23/samples/coco_00798.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_23/samples/coco_00237.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/apples.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/tree_1.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/flamingo.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/bear3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/bird.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/cat_3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/zebra.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/data/imagic_test/gray/landmark_image29231.jpg\"\n",
    "\n",
    "output_path = os.path.join(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs\", str(datetime.date.today()).replace(\"-\",\"_\"))\n",
    "cond_path = os.path.join(output_path, \"opti_cond_xloss.pt\")\n",
    "unet_path = os.path.join(output_path, \"opti_unet_xloss.pt\")\n",
    "sample_path = os.path.join(output_path, \"samples\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(output_path)) - 1\n",
    "torch.manual_seed(0)\n",
    "\n",
    "wm = \"StableDiffusionV1\"\n",
    "wm_encoder = WatermarkEncoder()\n",
    "wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n",
    "gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b8034a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest\n",
      "nf = 32, f = 32\n",
      "Network [SPADEGenerator] was created. Total number of parameters: 103.2 million. To see the architecture, do print(network).\n",
      "Network [NoVGGCorrespondence] was created. Total number of parameters: 50.8 million. To see the architecture, do print(network).\n",
      "Loading model from /home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-5-pruned.ckpt\n",
      "Global Step: 840000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'logit_scale', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "coco = get_cocosnet().to(device) \n",
    "# coco = get_cocosnetv2().to(device)\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "model = load_model_from_config(config, diffusion_model_path)\n",
    "model = model.to(device)\n",
    "\n",
    "vgg19_conv2_1_relu = models.vgg19(pretrained=True).features[:7].eval().to(device)\n",
    "for p in vgg19_conv2_1_relu.parameters():\n",
    "    p.requires_grad = False\n",
    "vgg19_conv3_1_relu = models.vgg19(pretrained=True).features[:11].eval().to(device)\n",
    "for p in vgg19_conv3_1_relu.parameters():\n",
    "    p.requires_grad = False\n",
    "# import torch  \n",
    "# from transformers import CLIPProcessor, CLIPModel  \n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "\n",
    "if False:\n",
    "    start_codenamed_parametersrandn([n_samples, 4, H // 8, W // 8], device=device)\n",
    "    \n",
    "precision_scope = autocast if True else nullcontext\n",
    "batch_size = 1\n",
    "n_rows = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad527f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load x_start\n"
     ]
    }
   ],
   "source": [
    "# neg_prompt = \"a gray photograph of a dog, a black bench in the center, and the grassland.\"\n",
    "neg_prompt = \"A grayscale photograph of a bird standing on a branch\"\n",
    "# neg_prompt = \"a mostly gray photograph.\"\n",
    "# prompt = \"a colorful photograph of a dog sitting on a bench on the grassland.\"\n",
    "#on a wooden bench, with the sky and the grassland\"\n",
    "# prompt = \"a colorful photograph of a yellow dog sitting on a bench, with the blue sky.\"\n",
    "# prompt = \"a colorful photograph of a dog sitting on a bench, with the sky and the grassland\"\n",
    "# prompt = \"a colorful photograph of a brown dog, sitting on a bench in the grassland\"\n",
    "# prompt = \"a colorful photograph of a dog in red\"\n",
    "# prompt = \"A colorful photograph of a red flamingo standing by the bule sea.\"\n",
    "# prompt = \"A photograph of a country road with the blue sky and green grassland\"\n",
    "# prompt = \"A colorful photograph of a Shiba Inu standing on the ground\"\n",
    "# prompt = \"a colorful photograph of a worn out teddy bear doll leaning against a fence.\"\n",
    "# prompt = \"a colorful photograph of a white car parked on the ground with the blue sky\"\n",
    "# prompt = \"a colorful photograph of an old green bus parked on the side of the road\"\n",
    "# prompt = \"a colorful photograph of an orange fox sits on a tree stump, behind is grassland\"\n",
    "# prompt = \"a colorful photograph of a flamingo walking in the blue seawater\"\n",
    "# prompt = \"a colorful photograph of a red telephone booth standing on the grass with a blue sky\"\n",
    "# prompt = \"a colorful photograph of a wooden chair resting on the sand with the blue ocean in the background\"\n",
    "# prompt = \"a colorful photograph of a white eagle spreading its wings and flying above the sea\"\n",
    "# prompt = \"a colorful photograph of heads of two horses close together, brown on the left and white on the right, with grass and trees in the background\"\n",
    "# prompt = \"a colorful photograph of a gray cat stands with yellow eyes, with a dark blue sky behind\"\n",
    "# prompt = \"a colorful photograph of a small brown wooden house in the forest, with white mist coming from the chimney, grass in front, snowy mountains in the distance\"\n",
    "# prompt = \"a colorful photograph of a yellow and white corgi, and a brown dog running on the dirt road in the warm sunshine\"\n",
    "# prompt = \"A golden retriever bouncing by the sea with white foam underfoot, blue water and red sky behind\"\n",
    "# prompt = \"a pink rose with green leaves in a transparent vase with water\"\n",
    "# prompt = \"a red sports car is parked in a parking lot, with the blue sky and white could\"\n",
    "# prompt = \"a polar bear is standing in the white snow, waving his hand, the sky is gray\"\n",
    "# prompt = \"a colorful photograph of a white horse running in the snow under the sky\"\n",
    "# prompt = \"a colorful photograph of a white horse running on the field with warm sunshine on it. \"\n",
    "# prompt = \"a small brown dog walking in the blue seawater.\"\n",
    "# prompt = \"a colorful photograph of a big red tree standing on the grassland under the purple sky\"\n",
    "# prompt = \"a yellow cat sitting before a red wall\"\n",
    "# prompt = \"a white cat sitting on the ground, background is the green grass\"\n",
    "# prompt = \"a green tennis ball on the floor of the tennis court\"\n",
    "# prompt = \"An Audi car is parked on the grass, mountains are in the distance, and the sky is blue\"\n",
    "# prompt = \"a colorful ohotograph of a worn out teddy brown bear doll leaning against a wooden board fence.\"\n",
    "# prompt = \"A colorful photograph of a basket of apples\"\n",
    "# prompt = \"a colorful photograph a big green tree on the meadow\"\n",
    "# prompt = \"A colorful photograph of a red flamingo standing by the sea.\"\n",
    "# prompt = \"A colorful photograph of a black bear walking in the green forest\"\n",
    "# prompt = \"A colorful photograph of a bird standing on a branch\"\n",
    "# prompt = \"a photograph of a sitting yellow cat\"\n",
    "# prompt = \"a colorful photograph of a bowl of red apples\"\n",
    "# prompt = \"a big white and red plane flying in the blue sky\"\n",
    "# prompt = \"a red bus driving on the street\"\n",
    "# prompt = \"a colorful photograph of a brown and white cow lies on a green grass\"\n",
    "# prompt = \"a colorful photograph.\"\n",
    "# prompt = \"A work of art with a purple lion standing on it, a red-haired woman lying on the ground, a yellow violin falling beside her, and a yellow moon in the sky.\"\n",
    "# prompt = \"A color photograph of houses with colorful roofs and a blue sky in the background\"\n",
    "# prompt = \"a colorful photograph of a black horse standing on a path in the countryside with a clear sky in the background.\"\n",
    "# resize_transform = transforms.Resize(512)  \n",
    "# pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "# resized_image = resize_transform(pil_image)  \n",
    "src_image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = src_image[None].transpose(0, 3, 1, 2)\n",
    "src_image = torch.from_numpy(src_image)\n",
    "\n",
    "src_image = src_image * 2.0 - 1.0\n",
    "src_image = src_image.to(device)\n",
    "\n",
    "un_cond = model.get_learned_conditioning(batch_size * [\"a grayscale photograph\"])\n",
    "ori_emb = model.get_learned_conditioning([prompt])\n",
    "src_image_latent = model.encode_first_stage(src_image)\n",
    "x_start = model.get_first_stage_encoding(src_image_latent)\n",
    "if os.path.isfile(\"x_start.pt\"):\n",
    "    print(\"load x_start\")\n",
    "    start_code = torch.load(\"x_start.pt\")\n",
    "else:\n",
    "    start_code = torch.randn_like(x_start)\n",
    "    torch.save(start_code, \"x_start.pt\")\n",
    "\n",
    "emb = ori_emb.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d54af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/gray_demo.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/flamingo.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/apples.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/52783230360_975280c68e_c.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/dog_01.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/teddy_1.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/white_horse2.png\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2341682248541_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/tree_1.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/flamingo.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/new_cat_3.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/tennis_ball.jpeg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/14913113756_31966e2e51_c.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2361682404477_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/jan-kohl-uDvpWXLd_7I-unsplash.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/alejandro-contreras-wTPp323zAEw-unsplash.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/zdenek-machacek-IJboLCiDwKY-unsplash.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/daniel-smith-_dNq7Mio_Qg-unsplash.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/zachary-kyra-derksen-ajqDp29Pz7M-unsplash.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2401682499774_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2411682509939_.pic.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/2421682511901_.pic_hd.jpg\"\n",
    "# gray_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/red_car.jpeg\"\n",
    "gray = np.array(Image.open(gray_path).convert(\"RGB\"))\n",
    "\n",
    "gray = gray.astype(np.float32) / 255.0\n",
    "gray = gray[None].transpose(0, 3, 1, 2)\n",
    "gray = torch.from_numpy(gray)\n",
    "\n",
    "gray = gray * 2.0 - 1.0\n",
    "gray = gray.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557fb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    #3. finetune diffusion model for reconstruction\n",
    "'''\n",
    "tt = time.time()\n",
    "print(\"-\" * 20, \"finetune for reconstruction\", \"-\" * 20)\n",
    "model.model.train()\n",
    "# model.model.eval()\n",
    "emb.requires_grad = False\n",
    "# optimizer_finetune = torch.optim.Adam([\n",
    "#                                         {'params':[emb], 'lr':1e-3},\n",
    "#                                         {'params':model.model.parameters(), 'lr':2e-7}], \n",
    "#                                         lr=1e-6)\n",
    "# optimizer_finetune = torch.optim.Adam([emb], lr=1e-3)\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=1.5e-6)\n",
    "print(\"cond.requires_grad=\",emb.requires_grad)\n",
    "print(\"cond_copy.requires_grad=\",ori_emb.requires_grad)\n",
    "# model.model.eval()\n",
    "# for p in model.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "steps = 1000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "# func =  Compose([\n",
    "#         Resize(256,interpolation=BICUBIC),\n",
    "#         CenterCrop(224),\n",
    "#         Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    # print(f\"t_tensor = {t_tensor}\")\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, emb)\n",
    "    pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    ''''''\n",
    "    # pred = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "    \n",
    "    pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # with torch.no_grad():\n",
    "    #     coco_x0_rgb = colorized(coco,src_image,pred_x0_rgb, False, output_path)\n",
    "    \n",
    "    # print(coco_x0_rgb.max(),coco_x0_rgb.min(),src_image.max(),src_image.min(),pred_x0_rgb.max(),pred_x0_rgb.min)\n",
    "    # x_sample = 255. * rearrange(((1.+pred_x0_rgb[0])/2.).detach().cpu().numpy(), 'c h w -> h w c')\n",
    "    # img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "    # img = put_watermark(img, wm_encoder)\n",
    "    # img.save(os.path.join(sample_path, f\"pred_{t_tensor.item()}.png\"))\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "        # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # pixel_loss =criteria(pred_x0_rgb, src_image)\n",
    "    # vgg_loss = vgg(pred_x0_rgb, src_image)\n",
    "    p_loss = vgg19_loss(src_image, pred_x0_rgb, vgg19_conv2_1_relu, vgg19_conv3_1_relu)\n",
    "    # coco_clip_loss = get_multi_clip_loss(clip_model, coco_x0_rgb, prompt)\n",
    "    pred_clip_loss = get_clip_loss(clip_model, pred_x0_rgb, prompt)\n",
    "    # pred_clip_loss = get_clip_loss_2(clip_model, processor, pred_x0_rgb.squeeze(0))\n",
    "    # w = (1- t_tensor.item() / model.num_timesteps)\n",
    "    # clip_loss = (0.5 * pred_clip_loss + 0.5 * coco_clip_loss)\n",
    "    clip_loss = 1. * pred_clip_loss\n",
    "    noise_loss = criteria(pred_noise, noise)\n",
    "    # if t_int>1500:\n",
    "    #     loss = .4 * clip_loss + 0.5 * noise_loss + 0.1 * p_loss\n",
    "    # else:\n",
    "    #     loss = .2 * clip_loss + 0.7 * noise_loss + 0.1 * p_loss\n",
    "    loss = 0.5 * clip_loss + 1. * noise_loss \n",
    "    #+ 0.5 * p_loss\n",
    "    # loss = clip_loss + 1. * noise_loss \n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    with torch.no_grad():\n",
    "        if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "            print(\"curr_t=\", t_int, \"; loss=\", loss.item(), f\"clip loss = {clip_loss.item()}, noise loss = {noise_loss.item()}, p loss = {p_loss.item()}\")\n",
    "            ret = generate_by_prompt(wm_encoder=wm_encoder, c=emb, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "            colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n",
    "            # colored_ret = colored_ret * 2.0 - 1.0\n",
    "            # x_start =  model.get_first_stage_encoding(model.encode_first_stage(colored_ret))\n",
    "\n",
    "print(time.time()-tt)\n",
    "unet_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_unet.pt\"\n",
    "\n",
    "print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "print(\"Done\\n\")\n",
    "\n",
    "cond_for_pixel_path = os.path.join(output_path,\"pixel_cond.pt\")\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "# torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22653b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp = os.path.join(output_path, \"first_stage_unet.pt\")\n",
    "# print(\"saving finetuned unet-model to path:\", ppp)\n",
    "# torch.save(model.model.state_dict(), ppp)\n",
    "# print(\"Done\\n\")\n",
    "\n",
    "model.model.load_state_dict(torch.load(ppp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cocosnetv2():\n",
    "#     with open('/home/v-penxiao/workspace/cocov2/CoCosNet-v2/cocov2_opt.pkl', 'rb') as f:\n",
    "#         opt = pickle.load(f)\n",
    "#     opt['checkpoints_dir'] = \"/home/v-penxiao/workspace/cocov2/CoCosNet-v2/checkpoints/\"\n",
    "#     opt = argparse.Namespace(**opt)\n",
    "#     opt.which_epoch = '5'\n",
    "#     print(opt.which_epoch)\n",
    "#     opt.name = 'XXX_deepfashionHD'\n",
    "#     # opt.name = 'deepfashionHD'\n",
    "#     model = Pix2PixModel(opt)\n",
    "#     model.eval()\n",
    "#     for p in model.parameters():\n",
    "#         p.requires_grad = False\n",
    "#     return model\n",
    "# coco = get_cocosnetv2().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ret = generate_by_prompt(wm_encoder=wm_encoder, c=emb, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "    colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_for_pixel_path = os.path.join(output_path,\"pixel_unet.pt\")\n",
    "\n",
    "print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "print(\"Done\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_for_pixel_path = os.path.join(output_path,\"pixel_unet.pt\")\n",
    "# model.model.load_state_dict(torch.load(unet_for_pixel_path))\n",
    "model.model.eval()\n",
    "emb.requires_grad = False\n",
    "with torch.no_grad():\n",
    "    # zero_emb = model.get_learned_conditioning([\"a colorful photograph of a red car driving fastly on the black street\"])\n",
    "    alpha = 1.1\n",
    "    new_c = emb * torch.Tensor([1-alpha]).to(device) + ori_emb * torch.Tensor([alpha]).to(device)\n",
    "    # new_cond = model.get_learned_conditioning([prompt + \"blue blue blue sky\"])\n",
    "    ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "    colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0966691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38859887",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttt=time.time()\n",
    "'''\n",
    "    #1. embedding optimization\n",
    "'''\n",
    "print(\"-\" * 20, \"embedding optimization\", \"-\" * 20)\n",
    "emb.requires_grad = True\n",
    "optimizer = torch.optim.Adam([emb], lr=1e-3)\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "steps = 500\n",
    "for t_int in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(1000, (1,), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, emb)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model(not available)'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "    \n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss = criteria(pred_noise, noise)\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer.step()\n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=emb, samplER=sampler, model=model, \n",
    "                                   output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, \n",
    "                                   n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_path)\n",
    "torch.save(emb, cond_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f57dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #2. finetune diffusion model\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune diffusion model\", \"-\" * 20)\n",
    "# cond = torch.load(cond_path)\n",
    "emb.requires_grad = False\n",
    "print(\"cond.requires_grad=\",emb.requires_grad)\n",
    "# model.model.train()\n",
    "model.train()\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=1e-6)\n",
    "steps = 1000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(model.num_timesteps, (1,), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, emb)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "      \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "    \n",
    "    # loss = 0.0 * criteria(pred_x0_gray_latent, x_start)  + 0. * criteria(pred_noise, noise) + 0. * vgg(pred_x0_gray, src_image)\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    loss = criteria(pred_noise, noise)\n",
    "\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    \n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=emb, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving finetuned unet-model to path:\", unet_path)\n",
    "torch.save(model.model.state_dict(), unet_path)\n",
    "print(\"Done\\n\")\n",
    "print(time.time()-ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b02358-80f8-4353-a6d4-46e19b602a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for alpha in (0.8, 0.9, 1, 1.1):\n",
    "    new_c = emb * torch.Tensor([1-alpha]).to(device) + ori_emb * torch.Tensor([alpha]).to(device)\n",
    "    ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # alpha = 1.1\n",
    "emb = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06ad601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:03<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"\\x1e2\\x87@\\xd3\\x97\\x07jZ)^c\\xe7M\\xc2'{} \\x08\\xabIC\\xd5\\xc0\\xead\\xcc\\x7f\\x85\\xc6\\x11S\\x16h\\xee\\xb2\\\\_X\\xe7\\xcc\\xcb\\xbd\\x043\\x98\\xf4\\x85y\\xcd\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xb6\\xcd\"]\n",
      "Bad pipe message: %s [b's\\xaa\\xdb\\xe6\\x1c.e#\\x87\\x0c$b\\xcd\\x8emAk.\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad']\n",
      "Bad pipe message: %s [b\"\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\", b'', b'']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b\"[D\\xf7\\xac\\xad\\xadl\\xd4+\\x1e\\xd0[\\xa8\\xe0c\\x02\\xfe\\xcc\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\", b'\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'^\\xcb\\x18T\\xa9\\xb0\\x96,\\xb7\\xb7\\xb2\\xc5,\\x8d%\\xa3', b'\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00', b'\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n']\n",
      "Bad pipe message: %s [b'\\xdcq\\xd6\\rG\\xe5\\xd6;,\\xea\\xa9\\xef\\xd1\\xa7\\xd7\\x11\\xb8\\xa1\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0']\n",
      "Bad pipe message: %s [b'\\x0c\\xc0\\x02\\x00\\x05\\x00']\n",
      "Bad pipe message: %s [b'\\xff\\x02\\x01']\n",
      "Bad pipe message: %s [b'\\x87k\\x93\\x1a\\xb0Q\\xd8yj\\xa4%!fP\\xe1VD\\xc2\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n']\n",
      "Bad pipe message: %s [b\"Yy8#\\x80\\x0f^8?ks:(\\x93\\xd9\\x96\\xb2\\xb9\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\"]\n",
      "Bad pipe message: %s [b\"\\xeb\\xb1H\\x94O*\\xe6\\xd9ok\\xceg\\xbb\\xb0\\xa0x8D\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00\", b'\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17']\n"
     ]
    }
   ],
   "source": [
    "# # alpha = 1.1\n",
    "# emb = torch.load(cond_path)\n",
    "# model.model.load_state_dict(torch.load(unet_path))\n",
    "\n",
    "model.eval()\n",
    "v = [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9]\n",
    "# zero_emb = ori_emb\n",
    "\n",
    "\n",
    "# zero_emb = model.get_learned_conditioning([\"A colorful photograph of a red flamingo standing by the blue sea.\"])\n",
    "# for alpha in v:\n",
    "# prompt = \"a colorful photograph of a white dog sitting on a wooden black bench, with the red sky and the green grassland\"\n",
    "# and the green grassland\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a red red red dog sitting on a red bench.\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a dog sitting on a bench, with the blue blue blue blue blue blue sky.\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a brown dog, sitting on a bench in the grassland\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a brown dog sitting on a red red red bench, with the purple purple purple purple purple purple sky\"])\n",
    "# zero_emb = ori_emb\n",
    "#  a yellow wooden boat, on the grassland, beside a small river with water, under the blue sky\n",
    "zero_emb = model.get_learned_conditioning([\"The very mint-green grassland under the very clear and blue sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a pipizhu red boat floating on the blue blue blue seawater, under the red sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"A bouquet of yellow flowers with green green green green green green stem in a clear glass vase on a wooden table, with bright lights\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"A red red red dog sitting next to a yellow canola flower field\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"A brown brown brown brown brown brown dog sitting on the ground, next to a canola flower field\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a pipizhu brown brown brown brown brown brown dog, sitting on a sibike purple bench, with the haohaoxiangguo green grassland, and the huadosousi red sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([ \"A blooming sunflower with bright bright bright bright bright yellow petals, under the blue sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"A blue teddy bear leaning against the red background\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"A green green green teddy bear leaning against the brown wooden board\"])\n",
    "# A pipizhu red telephone booth standing on the hhxg grassland, with the sibike sky\n",
    "# zero_emb = model.get_learned_conditioning([\"A worn out green bus parked on the side of the road, with the blue sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"A deep purple cabin with green bush, under the blue sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a blue teddy bear doll with red nose, leaning against a wooden board\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"Herd of brown horses standing on the grass by the lake with white clouds in the sky, the sky is red red red red red \"])\n",
    "# , with the purple sky\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a purple wooden windmill with pink fan leaves surrounded by green bushes and a blue blue blue blue blue blue sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a yellow dog sitting on a bench, with the red sky.\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a yellow dog walking on the beach with the blue sea water\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a worn out purple teddy bear doll leaning against a wooden board fence.\"])\n",
    "# zero_emb = ori_emb\n",
    "# zero_emb = model.get_learned_conditioning([\"A yellow retriever bouncing by the sea with white foam underfoot, blue water and very red red red sky behind\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a purple telephone booth; standing on the grass with a blue blue blue sky\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a colorful photograph of a yellow and white corgi, and a brown dog running on the dirt road in the warm sunshine\"])\n",
    "# zero_emb = model.get_learned_conditioning([\"a polar bear is standing in the white snow, waving his hand, the sky is gray\"])\n",
    "    # zero_emb = model.get_learned_conditioning([\"a colorful photograph of a yellow dog\" + \" with the black sky\"])\n",
    "# for alpha in v:\n",
    "alpha = 1.35\n",
    "# for alpha in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.5]:\n",
    "# # for alpha in []:\n",
    "new_c = emb * torch.Tensor([1-alpha]).to(device) + zero_emb * torch.Tensor([alpha]).to(device)\n",
    "ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)\n",
    "colored_ret = colorized(coco,gray,ret.type(gray.type()).to(gray.device), True, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cocoimagic_utils import *\n",
    "def get_cocosnetv2():\n",
    "    with open('/home/v-penxiao/workspace/cocov2/CoCosNet-v2/cocov2_opt.pkl', 'rb') as f:\n",
    "        opt = pickle.load(f)\n",
    "    opt['checkpoints_dir'] = \"/home/v-penxiao/workspace/cocov2/CoCosNet-v2/checkpoints/\"\n",
    "    opt = argparse.Namespace(**opt)\n",
    "    opt.which_epoch = '20'\n",
    "    print(opt.which_epoch)\n",
    "    opt.name = 'XXX_deepfashionHD'\n",
    "    # opt.name = 'deepfashionHD'\n",
    "    model = Pix2PixModel(opt)\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# def get_cocosnet():\n",
    "#     with open('/home/v-penxiao/workspace/cocosnet4imagic/CoCosNet/coco_opt.pkl', 'rb') as f:\n",
    "#         opt = pickle.load(f)\n",
    "#     opt['checkpoints_dir'] = \"/home/v-penxiao/workspace/cocosnet4imagic/CoCosNet/checkpoints/\"\n",
    "#     # opt['checkpoints_dir'] = \"/home/v-penxiao/workspace/cocov2/CoCosNet-v2/checkpoints/\"\n",
    "#     opt = argparse.Namespace(**opt)\n",
    "#     opt.which_epoch = '40'\n",
    "#     print(opt.which_epoch)\n",
    "#     opt.name = 'universal_512'\n",
    "#     # opt.name = 'deepfashionHD'\n",
    "#     model = Pix2PixModel(opt)\n",
    "#     model.eval()\n",
    "#     for p in model.parameters():\n",
    "#         p.requires_grad = False\n",
    "#     return model\n",
    "\n",
    "coco = get_cocosnetv2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8651955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "cond.requires_grad = False\n",
    "alpha = 0.9\n",
    "n = 3\n",
    "for _ in range(n):\n",
    "    with torch.no_grad():\n",
    "        new_c = cond * torch.Tensor([1-alpha]).to(device) + cond_copy * torch.Tensor([alpha]).to(device)\n",
    "        ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "cond.requires_grad = False\n",
    "\n",
    "v = [0.8, 0.85, 0.9, 1., 1.05, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7]\n",
    "n = 1\n",
    "# heights = [128 for _ in range(n)]\n",
    "# widths = [128 for _ in range(len(v))]\n",
    "# fig_width = n * 30  # inches\n",
    "# fig_height = fig_width * sum(heights) / sum(widths)\n",
    "# f, axarr = plt.subplots(len(v),n, figsize=(fig_width, fig_height),\n",
    "#                         gridspec_kw={'height_ratios':heights})\n",
    "for id, alpha in enumerate(v):\n",
    "    new_c = cond * torch.Tensor([1-alpha]).to(device) + cond_copy * torch.Tensor([alpha]).to(device)\n",
    "    for j in range(n):\n",
    "        with torch.no_grad():\n",
    "            ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2ceb9-965c-458f-a6cc-713771edd805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #3. finetune diffusion model for reconstruction\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune for reconstruction\", \"-\" * 20)\n",
    "model.model.train()\n",
    "optimizer_finetune = torch.optim.Adam([\n",
    "                                        {'params':[cond], 'lr':1e-5},\n",
    "                                        {'params':model.model.parameters(), 'lr':5e-7}], \n",
    "                                        lr=1e-6)\n",
    "# optimizer_finetune = torch.optim.Adam([cond], lr=2e-3)\n",
    "# optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=5e-8)\n",
    "cond.requires_grad = True\n",
    "print(\"cond.requires_grad=\",cond.requires_grad)\n",
    "# model.model.eval()\n",
    "# for p in model.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "steps = 3000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "# func =  Compose([\n",
    "#         Resize(256,interpolation=BICUBIC),\n",
    "#         CenterCrop(224),\n",
    "#         Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    \n",
    "    pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    coco_x0_rgb = colorized(coco,src_image,pred_x0_rgb, False, output_path)\n",
    "    \n",
    "        # print(coco_x0_rgb.max(),coco_x0_rgb.min(),src_image.max(),src_image.min(),pred_x0_rgb.max(),pred_x0_rgb.min)\n",
    "        # x_sample = 255. * rearrange(((1.+pred_x0_rgb[0])/2.).detach().cpu().numpy(), 'c h w -> h w c')\n",
    "        # img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "        # img = put_watermark(img, wm_encoder)\n",
    "        # img.save(os.path.join(sample_path, f\"coco_{t_int}.png\"))\n",
    "        # pred_x0_gray = degration(pred_x0_rgb)\n",
    "        # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "        # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # pixel_loss =criteria(pred_x0_rgb, src_image)\n",
    "    # vgg_loss = vgg(pred_x0_rgb, src_image)\n",
    "    # p_loss = vgg19_loss(src_image, pred_x0_rgb, vgg19_conv2_1_relu,vgg19_conv3_1_relu)\n",
    "    coco_clip_loss = get_clip_loss(clip_model, coco_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    pred_clip_loss = get_clip_loss(clip_model, pred_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    clip_loss = 1.0 * coco_clip_loss + 1. * pred_clip_loss\n",
    "    noise_loss = criteria(pred_noise, noise)\n",
    "    if t_int>2500:\n",
    "        loss = 1 * clip_loss + 0.1 * noise_loss \n",
    "    else:\n",
    "        loss = 0.2 * clip_loss + 1. * noise_loss \n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item(), f\"clip loss = {clip_loss.item()}, noise loss = {noise_loss.item()}\")\n",
    "        # new_cond = new_cond * torch.Tensor([1-0.1]).to(device) + model.get_learned_conditioning([prompt]) * torch.Tensor([0.1]).to(device)\n",
    "        ret = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "        colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n",
    "\n",
    "# unet_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_unet.pt\"\n",
    "\n",
    "# print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "# torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "# print(\"Done\\n\")\n",
    "\n",
    "cond_for_pixel_path = os.path.join(output_path,\"pixel_cond.pt\")\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "# torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_image = np.array(Image.open(\"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\").convert(\"RGB\"))\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = src_image[None].transpose(0, 3, 1, 2)\n",
    "src_image = torch.from_numpy(src_image)\n",
    "src_image = src_image * 2.0 - 1.0\n",
    "src_image = src_image.to(device)\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    \n",
    "func =  Compose([\n",
    "        Resize(256,interpolation=BICUBIC),\n",
    "        CenterCrop(224),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "# image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "src_image = (src_image+1.)/2.\n",
    "image = func(src_image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6f44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce15bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "neg_prompt = \"a grayscale photo\"\n",
    "# neg_prompt = \"a mostly gray photograph.\"\n",
    "prompt = \"a colorful photo\"\n",
    "# prompt = \"a colorful photograph.\"\n",
    "# image = preprocess(Image.open(\"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\")).unsqueeze(0).to(device)\n",
    "#/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/gray_demo.png\n",
    "image = preprocess(Image.open(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_03/samples/00252.png\")).unsqueeze(0).to(device)\n",
    "i_e = clip_model.encode_image(image)\n",
    "p_t_e = clip_model.encode_text(clip.tokenize(prompt).to(image.device))\n",
    "n_t_e = clip_model.encode_text(clip.tokenize(neg_prompt).to(image.device))\n",
    "g_t_e = clip_model.encode_text(clip.tokenize(\"grayscale\").to(image.device))\n",
    "# gray_t_e = model.encode_text(clip.tokenize(\"a gray photograph\").to(image.device))\n",
    "# color_t_e = model.encode_text(clip.tokenize(\"a colorful photograph\").to(image.device))\n",
    "p_cos_sim = torch.nn.functional.cosine_similarity(i_e, p_t_e).mean()\n",
    "n_cos_sim = torch.nn.functional.cosine_similarity(i_e, n_t_e).mean()\n",
    "g_cos_sim = torch.nn.functional.cosine_similarity(i_e, g_t_e).mean()\n",
    "print(p_cos_sim)\n",
    "print(n_cos_sim)\n",
    "print(g_cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77058401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69682d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(i_e, n_t_e).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(i_e, p_t_e).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.encode_text(clip.tokenize([prompt,neg_prompt]).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.tokenize(neg_prompt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2486a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94110260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  \n",
    "  \n",
    "# Read the image  \n",
    "# /home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/yuki-dog-E3GubD_EG7Y-unsplash.jpg\n",
    "# /home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/ingo-doerrie-Fkwj-xk6yck-unsplash.jpg\n",
    "# path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/2561682605505_.pic.jpg\"\n",
    "path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/3381683871697_.pic_hd.jpg\"\n",
    "image = Image.open(path)  \n",
    "  \n",
    "# Convert to grayscale  \n",
    "gray_image = image.convert('L')  \n",
    "# gray_image = image\n",
    "# Resize the image to 512x512  \n",
    "resized_image = gray_image.resize((512, 512))  \n",
    "  \n",
    "# Save the resized grayscale image  \n",
    "# resized_image.save(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/gray_2431682512060_.pic_hd.jpg\")  \n",
    "resized_image.save(path.replace(\"tests\", \"tests_gray\"))\n",
    "# resized_image.save(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/2741683031935_.pic_hd.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73f633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2284b64b7a38d4e06dbb5f84cc3277933c03c9b00bcd12a39d3717fb0a62a2d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
