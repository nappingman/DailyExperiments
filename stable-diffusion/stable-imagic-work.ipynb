{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007b9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apex not found\n",
      "apex not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CUDA:0 (NVIDIA A100 80GB PCIe, 80995MiB)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse, os, sys, glob\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "import torch  \n",
    "import torchvision.transforms as transforms  \n",
    "from PIL import Image  \n",
    "from vggloss import *\n",
    "from omegaconf import OmegaConf\n",
    "from imwatermark import WatermarkEncoder\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from cocoimagic_utils import *\n",
    "\n",
    "\n",
    "# environment\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# path\n",
    "experiment_time_stamp = datetime.date.today()\n",
    "config_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
    "diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-5-pruned.ckpt\"\n",
    "# diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-4-full-ema.ckpt\"\n",
    "img_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/gray_demo.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/flamingo.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/bear3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/bird.png\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/cat_3.jpeg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray/zebra.jpg\"\n",
    "# img_path = \"/home/v-penxiao/workspace/data/imagic_test/gray/landmark_image29231.jpg\"\n",
    "\n",
    "output_path = os.path.join(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs\", str(datetime.date.today()).replace(\"-\",\"_\"))\n",
    "cond_path = os.path.join(output_path, \"opti_cond_xloss.pt\")\n",
    "unet_path = os.path.join(output_path, \"opti_unet_xloss.pt\")\n",
    "sample_path = os.path.join(output_path, \"samples\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(output_path)) - 1\n",
    "torch.manual_seed(0)\n",
    "\n",
    "wm = \"StableDiffusionV1\"\n",
    "wm_encoder = WatermarkEncoder()\n",
    "wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n",
    "gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b8034a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf = 32, f = 32\n",
      "Network [SPADEGenerator] was created. Total number of parameters: 103.2 million. To see the architecture, do print(network).\n",
      "Network [NoVGGCorrespondence] was created. Total number of parameters: 50.8 million. To see the architecture, do print(network).\n",
      "Loading model from /home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-5-pruned.ckpt\n",
      "Global Step: 840000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'text_projection.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'logit_scale', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "coco = get_cocosnet().to(device)\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "model = load_model_from_config(config, diffusion_model_path)\n",
    "model = model.to(device)\n",
    "\n",
    "vgg19_conv2_1_relu = models.vgg19(pretrained=True).features[:7].eval().to(device)\n",
    "for p in vgg19_conv2_1_relu.parameters():\n",
    "    p.requires_grad = False\n",
    "vgg19_conv3_1_relu = models.vgg19(pretrained=True).features[:11].eval().to(device)\n",
    "for p in vgg19_conv3_1_relu.parameters():\n",
    "    p.requires_grad = False\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "if False:\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "start_code = None\n",
    "\n",
    "if False:\n",
    "    start_codenamed_parametersrandn([n_samples, 4, H // 8, W // 8], device=device)\n",
    "    \n",
    "precision_scope = autocast if True else nullcontext\n",
    "batch_size = 1\n",
    "n_rows = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad527f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_prompt = \"a gray photograph of a dog, a black bench in the center, and the grassland.\"\n",
    "neg_prompt = \"A grayscale photograph of a bird standing on a branch\"\n",
    "# neg_prompt = \"a mostly gray photograph.\"\n",
    "# prompt = \"a colorful photograph of a yellow dog,  a dark red bench, the bule sky and the green grassland\"\n",
    "prompt = \"a yellow dog, sitting on a bench in the grassland\"\n",
    "# prompt = \"A colorful photograph of a red flamingo standing by the sea.\"\n",
    "# prompt = \"A colorful photograph of a black bear walking in the green forest\"\n",
    "# prompt = \"A colorful photograph of a bird standing on a branch\"\n",
    "# prompt = \"a photograph of a sitting yellow cat\"\n",
    "# prompt = \"a colorful photograph of a bowl of red apples\"\n",
    "# prompt = \"a big white and red plane flying in the blue sky\"\n",
    "# prompt = \"a red bus driving on the street\"\n",
    "# prompt = \"a colorful photograph of a brown and white cow lies on a green grass\"\n",
    "# prompt = \"a colorful photograph.\"\n",
    "# prompt = \"A work of art with a purple lion standing on it, a red-haired woman lying on the ground, a yellow violin falling beside her, and a yellow moon in the sky.\"\n",
    "# prompt = \"A color photograph of houses with colorful roofs and a blue sky in the background\"\n",
    "# prompt = \"a colorful photograph of a black horse standing on a path in the countryside with a clear sky in the background.\"\n",
    "# resize_transform = transforms.Resize(512)  \n",
    "# pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "# resized_image = resize_transform(pil_image)  \n",
    "src_image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = src_image[None].transpose(0, 3, 1, 2)\n",
    "src_image = torch.from_numpy(src_image)\n",
    "\n",
    "src_image = src_image * 2.0 - 1.0\n",
    "src_image = src_image.to(device)\n",
    "\n",
    "un_cond = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "cond = model.get_learned_conditioning([prompt])\n",
    "src_image_latent = model.encode_first_stage(src_image)\n",
    "x_start = model.get_first_stage_encoding(src_image_latent)\n",
    "\n",
    "cond.requires_grad = False\n",
    "cond_copy = cond.clone()\n",
    "x_start.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557fb610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- finetune for reconstruction --------------------\n",
      "cond.requires_grad= False\n",
      "cond_copy.requires_grad= False\n",
      "t_tensor = tensor([240], device='cuda:0')\n",
      "t_tensor = tensor([195], device='cuda:0')\n",
      "t_tensor = tensor([325], device='cuda:0')\n",
      "t_tensor = tensor([866], device='cuda:0')\n",
      "t_tensor = tensor([189], device='cuda:0')\n",
      "t_tensor = tensor([366], device='cuda:0')\n",
      "t_tensor = tensor([11], device='cuda:0')\n",
      "t_tensor = tensor([47], device='cuda:0')\n",
      "t_tensor = tensor([174], device='cuda:0')\n",
      "t_tensor = tensor([312], device='cuda:0')\n",
      "t_tensor = tensor([515], device='cuda:0')\n",
      "t_tensor = tensor([663], device='cuda:0')\n",
      "t_tensor = tensor([471], device='cuda:0')\n",
      "t_tensor = tensor([229], device='cuda:0')\n",
      "t_tensor = tensor([788], device='cuda:0')\n",
      "t_tensor = tensor([669], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/PIL/ImageFile.py:503\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mfileno()\n\u001b[1;32m    504\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(x_sample\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8))\n\u001b[1;32m     51\u001b[0m img \u001b[39m=\u001b[39m put_watermark(img, wm_encoder)\n\u001b[0;32m---> 52\u001b[0m img\u001b[39m.\u001b[39;49msave(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(sample_path, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpred_\u001b[39;49m\u001b[39m{\u001b[39;49;00mt_tensor\u001b[39m.\u001b[39;49mitem()\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     53\u001b[0m \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# pred_x0_gray = degration(pred_x0_rgb)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m# p_loss = vgg19_loss(src_image, pred_x0_rgb, vgg19_conv2_1_relu, vgg19_conv3_1_relu)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# coco_clip_loss = get_clip_loss(clip_model, coco_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/PIL/Image.py:2353\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2353\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[1;32m   2354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   2355\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/PIL/PngImagePlugin.py:1397\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1396\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1397\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)])\n\u001b[1;32m   1399\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[1;32m   1400\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/PIL/ImageFile.py:507\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    505\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    506\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 507\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;49;00m, exc)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(fp, \u001b[39m\"\u001b[39m\u001b[39mflush\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    509\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/PIL/ImageFile.py:526\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m exc:\n\u001b[1;32m    524\u001b[0m     \u001b[39m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m         l, s, d \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(bufsize)\n\u001b[1;32m    527\u001b[0m         fp\u001b[39m.\u001b[39mwrite(d)\n\u001b[1;32m    528\u001b[0m         \u001b[39mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    #3. finetune diffusion model for reconstruction\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune for reconstruction\", \"-\" * 20)\n",
    "model.model.train()\n",
    "cond.requires_grad = False\n",
    "# optimizer_finetune = torch.optim.Adam([\n",
    "#                                         {'params':[cond], 'lr':1e-5},\n",
    "#                                         {'params':model.model.parameters(), 'lr':2e-7}], \n",
    "#                                         lr=1e-6)\n",
    "# optimizer_finetune = torch.optim.Adam([cond], lr=2e-3)\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=2e-7)\n",
    "print(\"cond.requires_grad=\",cond.requires_grad)\n",
    "print(\"cond_copy.requires_grad=\",cond_copy.requires_grad)\n",
    "# model.model.eval()\n",
    "# for p in model.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "steps = 3500\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "# func =  Compose([\n",
    "#         Resize(256,interpolation=BICUBIC),\n",
    "#         CenterCrop(224),\n",
    "#         Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    print(f\"t_tensor = {t_tensor}\")\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    ''''''\n",
    "    # pred = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "    \n",
    "    pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # coco_x0_rgb = colorized(coco,src_image,pred_x0_rgb, True, output_path)\n",
    "    \n",
    "        # print(coco_x0_rgb.max(),coco_x0_rgb.min(),src_image.max(),src_image.min(),pred_x0_rgb.max(),pred_x0_rgb.min)\n",
    "    x_sample = 255. * rearrange(((1.+pred_x0_rgb[0])/2.).detach().cpu().numpy(), 'c h w -> h w c')\n",
    "    img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "    img = put_watermark(img, wm_encoder)\n",
    "    img.save(os.path.join(sample_path, f\"pred_{t_tensor.item()}.png\"))\n",
    "    continue\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "        # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # pixel_loss =criteria(pred_x0_rgb, src_image)\n",
    "    # vgg_loss = vgg(pred_x0_rgb, src_image)\n",
    "    # p_loss = vgg19_loss(src_image, pred_x0_rgb, vgg19_conv2_1_relu, vgg19_conv3_1_relu)\n",
    "    # coco_clip_loss = get_clip_loss(clip_model, coco_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    pred_clip_loss = get_clip_loss(clip_model, pred_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    # clip_loss = .5 * pred_clip_loss + .5 * coco_clip_loss\n",
    "    clip_loss = 1. * pred_clip_loss\n",
    "    noise_loss = criteria(pred_noise, noise)\n",
    "    # if t_int>1500:\n",
    "    #     loss = .4 * clip_loss + 0.5 * noise_loss + 0.1 * p_loss\n",
    "    # else:\n",
    "    #     loss = .2 * clip_loss + 0.7 * noise_loss + 0.1 * p_loss\n",
    "    loss = .2 * clip_loss + 0.8 * noise_loss \n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    with torch.no_grad():\n",
    "        if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "            print(\"curr_t=\", t_int, \"; loss=\", loss.item(), f\"clip loss = {clip_loss.item()}, noise loss = {noise_loss.item()}\")\n",
    "            ret = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "            colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n",
    "    # x_start =  model.get_first_stage_encoding(model.encode_first_stage(coco_x0_rgb))\n",
    "\n",
    "# unet_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_unet.pt\"\n",
    "\n",
    "# print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "# torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "# print(\"Done\\n\")\n",
    "\n",
    "cond_for_pixel_path = os.path.join(output_path,\"pixel_cond.pt\")\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "# torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f455a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m         ret \u001b[39m=\u001b[39m generate_by_prompt(wm_encoder\u001b[39m=\u001b[39mwm_encoder, c\u001b[39m=\u001b[39mcond, samplER\u001b[39m=\u001b[39msampler, model\u001b[39m=\u001b[39mmodel, output_path\u001b[39m=\u001b[39moutput_path, start_code\u001b[39m=\u001b[39mstart_code,h\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, w\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, ddim_eta\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, n_samples\u001b[39m=\u001b[39mbatch_size, scale\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, ddim_steps\u001b[39m=\u001b[39m\u001b[39m45\u001b[39m)\n\u001b[1;32m      4\u001b[0m         colored_ret \u001b[39m=\u001b[39m colorized(coco,src_image,ret\u001b[39m.\u001b[39mtype(src_image\u001b[39m.\u001b[39mtype())\u001b[39m.\u001b[39mto(src_image\u001b[39m.\u001b[39mdevice), \u001b[39mTrue\u001b[39;00m, output_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        ret = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "        colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38859887",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #1. embedding optimization\n",
    "'''\n",
    "print(\"-\" * 20, \"embedding optimization\", \"-\" * 20)\n",
    "cond.requires_grad = True\n",
    "optimizer = torch.optim.Adam([cond], lr=2e-3)\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "steps = 1000\n",
    "for t_int in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model(not available)'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "    \n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss = criteria(pred_noise, noise)\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer.step()\n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, \n",
    "                                   output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, \n",
    "                                   n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_path)\n",
    "torch.save(cond, cond_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f57dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #2. finetune diffusion model\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune diffusion model\", \"-\" * 20)\n",
    "# cond = torch.load(cond_path)\n",
    "cond.requires_grad = False\n",
    "print(\"cond.requires_grad=\",cond.requires_grad)\n",
    "model.model.train()\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=5e-7)\n",
    "steps = 2000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "      \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "    \n",
    "    # loss = 0.0 * criteria(pred_x0_gray_latent, x_start)  + 0. * criteria(pred_noise, noise) + 0. * vgg(pred_x0_gray, src_image)\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    loss = criteria(pred_noise, noise)\n",
    "\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    \n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving finetuned unet-model to path:\", unet_path)\n",
    "torch.save(model.model.state_dict(), unet_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b02358-80f8-4353-a6d4-46e19b602a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "cond.requires_grad = False\n",
    "v = [0.8, 0.85, 0.9, 1., 1.05, 1.1, 1.2, 1.3, 1.4]\n",
    "n = 3\n",
    "# heights = [128 for _ in range(n)]\n",
    "# widths = [128 for _ in range(len(v))]\n",
    "# fig_width = n * 30  # inches\n",
    "# fig_height = fig_width * sum(heights) / sum(widths)\n",
    "# f, axarr = plt.subplots(len(v),n, figsize=(fig_width, fig_height),\n",
    "#                         gridspec_kw={'height_ratios':heights})\n",
    "for id, alpha in enumerate(v):\n",
    "    new_c = cond * torch.Tensor([1-alpha]).to(device) + cond * torch.Tensor([alpha]).to(device)\n",
    "    for j in range(n):\n",
    "        with torch.no_grad():\n",
    "            ret = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2ceb9-965c-458f-a6cc-713771edd805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    #3. finetune diffusion model for reconstruction\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune for reconstruction\", \"-\" * 20)\n",
    "model.model.train()\n",
    "optimizer_finetune = torch.optim.Adam([\n",
    "                                        {'params':[cond], 'lr':1e-5},\n",
    "                                        {'params':model.model.parameters(), 'lr':5e-7}], \n",
    "                                        lr=1e-6)\n",
    "# optimizer_finetune = torch.optim.Adam([cond], lr=2e-3)\n",
    "# optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=5e-8)\n",
    "cond.requires_grad = True\n",
    "print(\"cond.requires_grad=\",cond.requires_grad)\n",
    "# model.model.eval()\n",
    "# for p in model.model.parameters():\n",
    "#     p.requires_grad = False\n",
    "steps = 3000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "# func =  Compose([\n",
    "#         Resize(256,interpolation=BICUBIC),\n",
    "#         CenterCrop(224),\n",
    "#         Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "#     ])\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    \n",
    "    pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    coco_x0_rgb = colorized(coco,src_image,pred_x0_rgb, False, output_path)\n",
    "    \n",
    "        # print(coco_x0_rgb.max(),coco_x0_rgb.min(),src_image.max(),src_image.min(),pred_x0_rgb.max(),pred_x0_rgb.min)\n",
    "        # x_sample = 255. * rearrange(((1.+pred_x0_rgb[0])/2.).detach().cpu().numpy(), 'c h w -> h w c')\n",
    "        # img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "        # img = put_watermark(img, wm_encoder)\n",
    "        # img.save(os.path.join(sample_path, f\"coco_{t_int}.png\"))\n",
    "        # pred_x0_gray = degration(pred_x0_rgb)\n",
    "        # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "        # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # pixel_loss =criteria(pred_x0_rgb, src_image)\n",
    "    # vgg_loss = vgg(pred_x0_rgb, src_image)\n",
    "    # p_loss = vgg19_loss(src_image, pred_x0_rgb, vgg19_conv2_1_relu,vgg19_conv3_1_relu)\n",
    "    coco_clip_loss = get_clip_loss(clip_model, coco_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    pred_clip_loss = get_clip_loss(clip_model, pred_x0_rgb, pos_prompt=prompt, neg_prompt=neg_prompt)\n",
    "    clip_loss = 1.0 * coco_clip_loss + 1. * pred_clip_loss\n",
    "    noise_loss = criteria(pred_noise, noise)\n",
    "    if t_int>2500:\n",
    "        loss = 1 * clip_loss + 0.1 * noise_loss \n",
    "    else:\n",
    "        loss = 0.2 * clip_loss + 1. * noise_loss \n",
    "    # loss = criteria(pred_x0_rgb, colorized(coco,src_image,pred_x0_rgb))\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item(), f\"clip loss = {clip_loss.item()}, noise loss = {noise_loss.item()}\")\n",
    "        # new_cond = new_cond * torch.Tensor([1-0.1]).to(device) + model.get_learned_conditioning([prompt]) * torch.Tensor([0.1]).to(device)\n",
    "        ret = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, output_path=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "        colored_ret = colorized(coco,src_image,ret.type(src_image.type()).to(src_image.device), True, output_path)\n",
    "\n",
    "# unet_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_unet.pt\"\n",
    "\n",
    "# print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "# torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "# print(\"Done\\n\")\n",
    "\n",
    "cond_for_pixel_path = os.path.join(output_path,\"pixel_cond.pt\")\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "# torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_image = np.array(Image.open(\"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\").convert(\"RGB\"))\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = src_image[None].transpose(0, 3, 1, 2)\n",
    "src_image = torch.from_numpy(src_image)\n",
    "src_image = src_image * 2.0 - 1.0\n",
    "src_image = src_image.to(device)\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    \n",
    "func =  Compose([\n",
    "        Resize(256,interpolation=BICUBIC),\n",
    "        CenterCrop(224),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "# image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "src_image = (src_image+1.)/2.\n",
    "image = func(src_image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6f44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce15bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "neg_prompt = \"a grayscale photo\"\n",
    "# neg_prompt = \"a mostly gray photograph.\"\n",
    "prompt = \"a colorful photo\"\n",
    "# prompt = \"a colorful photograph.\"\n",
    "# image = preprocess(Image.open(\"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\")).unsqueeze(0).to(device)\n",
    "#/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/gray_demo.png\n",
    "image = preprocess(Image.open(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/2023_04_03/samples/00252.png\")).unsqueeze(0).to(device)\n",
    "i_e = clip_model.encode_image(image)\n",
    "p_t_e = clip_model.encode_text(clip.tokenize(prompt).to(image.device))\n",
    "n_t_e = clip_model.encode_text(clip.tokenize(neg_prompt).to(image.device))\n",
    "g_t_e = clip_model.encode_text(clip.tokenize(\"grayscale\").to(image.device))\n",
    "# gray_t_e = model.encode_text(clip.tokenize(\"a gray photograph\").to(image.device))\n",
    "# color_t_e = model.encode_text(clip.tokenize(\"a colorful photograph\").to(image.device))\n",
    "p_cos_sim = torch.nn.functional.cosine_similarity(i_e, p_t_e).mean()\n",
    "n_cos_sim = torch.nn.functional.cosine_similarity(i_e, n_t_e).mean()\n",
    "g_cos_sim = torch.nn.functional.cosine_similarity(i_e, g_t_e).mean()\n",
    "print(p_cos_sim)\n",
    "print(n_cos_sim)\n",
    "print(g_cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77058401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69682d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(i_e, n_t_e).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(i_e, p_t_e).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.encode_text(clip.tokenize([prompt,neg_prompt]).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.tokenize(neg_prompt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "directory_in_str = '/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests'\n",
    "output_directory = '/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests_gray'\n",
    "\n",
    "directory = os.fsencode(directory_in_str)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "        filepath = os.path.join(directory_in_str, filename)\n",
    "        new_filepath = os.path.join(output_directory, filename)\n",
    "        im = Image.open(filepath).convert('L').resize((512, 512))\n",
    "        im.save(new_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "src_image = np.array(Image.open(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs/gray_demo.png\").convert(\"RGB\"))\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = torch.from_numpy(src_image).permute(2,0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11269127",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "sam = sam_model_registry[\"default\"](checkpoint=\"/home/v-penxiao/workspace/DailyCkpts/sam/sam_vit_h_4b8939.pth\")\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(src_image)\n",
    "masks, _, _ = predictor.predict(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(src_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "brightness_change = transforms.ColorJitter(brightness=0.1)\n",
    "# \n",
    "hue_change = transforms.ColorJitter(hue=0.1)\n",
    "# \n",
    "contrast_change = transforms.ColorJitter(contrast=0.1)\n",
    "\n",
    "# \n",
    "color_aug = transforms.ColorJitter(brightness=0.3, contrast=0.5, saturation=0.1, hue=0.3)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        # brightness_change,\n",
    "        # hue_change,\n",
    "        # contrast_change,\n",
    "        color_aug\n",
    "    ])\n",
    "# img_path = \"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/tests/apples.jpeg\"\n",
    "img_path = \"/home/v-penxiao/workspace/cocosnet4imagic/CoCosNet/before.jpg\"\n",
    "src_image = cv2.imread(img_path)\n",
    "color = cv2.cvtColor(src_image, cv2.COLOR_BGR2RGB)\n",
    "color = Image.fromarray(color)\n",
    "color = transform(color)\n",
    "color = np.asarray(color)\n",
    "cv2.imwrite(\"yy.jpg\", color)\n",
    "# color = cv.cvtColor(color, cv.COLOR_RGB2BGR)\n",
    "# save_image(src_image, \"yy.jpg\")\n",
    "# save_image(color, \"xx.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ccb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = LineCollator(img_size=src_image.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2486a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2abe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "def uniform_grid(shape):\n",
    "    '''Uniform grid coordinates.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    shape : tuple\n",
    "        HxW defining the number of height and width dimension of the grid\n",
    "    Returns\n",
    "    -------\n",
    "    points: HxWx2 tensor\n",
    "        Grid coordinates over [0,1] normalized image range.\n",
    "    '''\n",
    "\n",
    "    H, W = shape[:2]\n",
    "    c = np.empty((H, W, 2))\n",
    "    c[..., 0] = np.linspace(0, 1, W, dtype=np.float32)\n",
    "    c[..., 1] = np.expand_dims(np.linspace(0, 1, H, dtype=np.float32), -1)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def tps_theta_from_points(c_src, c_dst, reduced=False):\n",
    "    delta = c_src - c_dst\n",
    "    \n",
    "    cx = np.column_stack((c_dst, delta[:, 0]))\n",
    "    cy = np.column_stack((c_dst, delta[:, 1]))\n",
    "        \n",
    "    theta_dx = TPS.fit(cx, reduced=reduced)\n",
    "    theta_dy = TPS.fit(cy, reduced=reduced)\n",
    "\n",
    "    return np.stack((theta_dx, theta_dy), -1)\n",
    "\n",
    "\n",
    "def tps_grid(theta, c_dst, dshape):\n",
    "    ugrid = uniform_grid(dshape)\n",
    "\n",
    "    reduced = c_dst.shape[0] + 2 == theta.shape[0]\n",
    "\n",
    "    dx = TPS.z(ugrid.reshape((-1, 2)), c_dst, theta[:, 0]).reshape(dshape[:2])\n",
    "    dy = TPS.z(ugrid.reshape((-1, 2)), c_dst, theta[:, 1]).reshape(dshape[:2])\n",
    "    dgrid = np.stack((dx, dy), -1)\n",
    "\n",
    "    grid = dgrid + ugrid\n",
    "    \n",
    "    return grid # H'xW'x2 grid[i,j] in range [0..1]\n",
    "\n",
    "\n",
    "def tps_grid_to_remap(grid, sshape):\n",
    "    '''Convert a dense grid to OpenCV's remap compatible maps.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    grid : HxWx2 array\n",
    "        Normalized flow field coordinates as computed by compute_densegrid.\n",
    "    sshape : tuple\n",
    "        Height and width of source image in pixels.\n",
    "    Returns\n",
    "    -------\n",
    "    mapx : HxW array\n",
    "    mapy : HxW array\n",
    "    '''\n",
    "\n",
    "    mx = (grid[:, :, 0] * sshape[1]).astype(np.float32)\n",
    "    my = (grid[:, :, 1] * sshape[0]).astype(np.float32)\n",
    "\n",
    "    return mx, my\n",
    "\n",
    "\n",
    "def warping_image(img, c_src, c_dst):\n",
    "    dshape = img.shape\n",
    "    theta = tps_theta_from_points(c_src, c_dst, reduced=True)\n",
    "    grid = tps_grid(theta, c_dst, dshape)\n",
    "    mapx, mapy = tps_grid_to_remap(grid, img.shape)\n",
    "    \n",
    "    return cv.remap(img, mapx, mapy, cv.INTER_CUBIC)\n",
    "\n",
    "class LineCollator:\n",
    "    \"\"\"Collator for training.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 src_perturbation=0.5,\n",
    "                 dst_perturbation=0.2,\n",
    "                 brightness=0.3,\n",
    "                 contrast=0.5,\n",
    "                 saturation=0.1,\n",
    "                 hue=0.3):\n",
    "\n",
    "        self.size = img_size\n",
    "        self.src_per = src_perturbation\n",
    "        self.tgt_per = dst_perturbation\n",
    "        self.thre = 50\n",
    "\n",
    "        self.src_const = np.array([\n",
    "            [-0.5, -0.5],\n",
    "            [0.5, -0.5],\n",
    "            [-0.5, 0.5],\n",
    "            [0.5, 0.5],\n",
    "            [0.2, -0.2],\n",
    "            [-0.2, 0.2],\n",
    "            [0.2, 0.2],\n",
    "            [-0.2, -0.2]\n",
    "        ])\n",
    "\n",
    "        self.jittering = ColorJitter(brightness, contrast, saturation, hue)\n",
    "\n",
    "    @staticmethod\n",
    "    def _random_crop(line, color, size):\n",
    "        height, width = line.shape[0], line.shape[1]\n",
    "        rnd0 = np.random.randint(height - size - 1)\n",
    "        rnd1 = np.random.randint(width - size - 1)\n",
    "\n",
    "        line = line[rnd0: rnd0 + size, rnd1: rnd1 + size]\n",
    "        color = color[rnd0: rnd0 + size, rnd1: rnd1 + size]\n",
    "\n",
    "        return line, color\n",
    "\n",
    "    @staticmethod\n",
    "    def _coordinate(image):\n",
    "        \"\"\"3 stage of manipulation\n",
    "           - BGR -> RGB\n",
    "           - (H, W, C) -> (C, H, W)\n",
    "           - Normalize\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        image : numpy.array\n",
    "            image data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            manipulated image data\n",
    "        \"\"\"\n",
    "        image = image[:, :, ::-1]\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        image = (image - 127.5) / 127.5\n",
    "\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def _totensor(array_list):\n",
    "        array = np.array(array_list).astype(np.float32)\n",
    "        tensor = torch.FloatTensor(array)\n",
    "        tensor = tensor.cuda()\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def _warp(self, img):\n",
    "        \"\"\"Spatial augment by TPS\n",
    "        \"\"\"\n",
    "        const = self.src_const\n",
    "        c_src = const + np.random.uniform(-self.src_per, self.src_per, (8, 2))\n",
    "        c_tgt = c_src + np.random.uniform(-self.tgt_per, self.tgt_per, (8, 2))\n",
    "\n",
    "        img = warping_image(img, c_src, c_tgt)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _jitter(self, img):\n",
    "        \"\"\"Color augment\n",
    "        \"\"\"\n",
    "        img = img.astype(np.float32)\n",
    "        noise = np.random.uniform(-self.thre, self.thre)\n",
    "        img += noise\n",
    "        img = np.clip(img, 0, 255)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _prepair(self, color, line):\n",
    "        \"\"\"3 stages of preparation\n",
    "           - Crop\n",
    "           - Spatial & Color augmentation\n",
    "           - Coordination\n",
    "        \"\"\"\n",
    "        line, color = self._random_crop(line, color, size=self.size)\n",
    "\n",
    "        jittered = self._jitter(color)\n",
    "        warped = self._warp(jittered)\n",
    "\n",
    "        jittered = self._coordinate(jittered)\n",
    "        warped = self._coordinate(warped)\n",
    "        line = self._coordinate(line)\n",
    "\n",
    "        return jittered, warped, line\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        j_box = []\n",
    "        w_box = []\n",
    "        l_box = []\n",
    "\n",
    "        for b in batch:\n",
    "            color, line = b\n",
    "            jitter, warped, line = self._prepair(color, line)\n",
    "\n",
    "            j_box.append(jitter)\n",
    "            w_box.append(warped)\n",
    "            l_box.append(line)\n",
    "\n",
    "        j = self._totensor(j_box)\n",
    "        w = self._totensor(w_box)\n",
    "        l = self._totensor(l_box)\n",
    "\n",
    "        return (j, w, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94110260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2284b64b7a38d4e06dbb5f84cc3277933c03c9b00bcd12a39d3717fb0a62a2d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
