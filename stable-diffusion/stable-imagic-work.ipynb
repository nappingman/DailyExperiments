{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007b9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from vggloss import *\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "from torch import autocast\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm, trange\n",
    "from omegaconf import OmegaConf\n",
    "from imwatermark import WatermarkEncoder\n",
    "from itertools import islice\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import seed_everything\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "# environment\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# helper\n",
    "toTensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "# path\n",
    "experiment_time_stamp = datetime.date.today()\n",
    "config_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
    "# diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/v1-5-pruned.ckpt\"\n",
    "diffusion_model_path = \"/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/sd-v1-4-full-ema.ckpt \"\n",
    "img_path = \"/home/v-penxiao/workspace/DailyExperiments/stable-diffusion/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "output_path = os.path.join(\"/home/v-penxiao/workspace/DailyOutputs/stable_outputs/imagic_outputs\", str(datetime.date.today()).replace(\"-\",\"_\"))\n",
    "cond_path = os.path.join(output_path, \"opti_cond_xloss.pt\")\n",
    "unet_path = os.path.join(output_path, \"opti_unet_xloss.pt\")\n",
    "sample_path = os.path.join(output_path, \"samples\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(output_path)) - 1\n",
    "torch.manual_seed(0)\n",
    "wm = \"StableDiffusionV1\"\n",
    "wm_encoder = WatermarkEncoder()\n",
    "wm_encoder.set_watermark('bytes', wm.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12385cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_info() -> str:\n",
    "    info = ''\n",
    "    for id in range(torch.cuda.device_count()):\n",
    "        p = torch.cuda.get_device_properties(id)\n",
    "        info += f'CUDA:{id} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\\n'\n",
    "    return info[:-1]\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def numpy_to_pil(images):\n",
    "    \"\"\"\n",
    "    Convert a numpy image or a batch of images to a PIL image.\n",
    "    \"\"\"\n",
    "    if images.ndim == 3:\n",
    "        images = images[None, ...]\n",
    "    images = (images * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "    return pil_images\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def put_watermark(img, wm_encoder=None):\n",
    "    if wm_encoder is not None:\n",
    "        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        img = wm_encoder.encode(img, 'dwtDct')\n",
    "        img = Image.fromarray(img[:, :, ::-1])\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_replacement(x):\n",
    "    try:\n",
    "        hwc = x.shape\n",
    "        y = Image.open(\"assets/rick.jpeg\").convert(\"RGB\").resize((hwc[1], hwc[0]))\n",
    "        y = (np.array(y)/255.0).astype(x.dtype)\n",
    "        assert y.shape == x.shape\n",
    "        return y\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "\n",
    "def check_safety(x_image):\n",
    "    return x_image, False\n",
    "\n",
    "def generate_by_prompt(wm_encoder, c, samplER, model, outpath, start_code, h=256, w=256, ddim_eta=0.0, n_samples=1, scale=7.5,\n",
    "                       ddim_steps=50):\n",
    "    \n",
    "    sample_path = os.path.join(output_path, \"samples\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "    grid_count = len(os.listdir(output_path)) - 1\n",
    "    all_samples = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                uc = None\n",
    "                if scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "#                 c = model.get_learned_conditioning([prompt])\n",
    "                shape = [4, h // 8, w // 8]\n",
    "                samples_ddim, _ = samplER.sample(S=ddim_steps,\n",
    "                                                 conditioning=c,\n",
    "                                                 batch_size=n_samples,\n",
    "                                                 shape=shape,\n",
    "                                                 verbose=False,\n",
    "                                                 unconditional_guidance_scale=scale,\n",
    "                                                 unconditional_conditioning=uc,\n",
    "                                                 eta=ddim_eta,\n",
    "                                                 x_T=start_code)\n",
    "                \n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "                x_checked_image, has_nsfw_concept = check_safety(x_samples_ddim)\n",
    "                x_checked_image_torch = torch.from_numpy(x_checked_image).permute(0, 3, 1, 2)\n",
    "\n",
    "                if True:\n",
    "                    for x_sample in x_checked_image_torch:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        img = put_watermark(img, wm_encoder)\n",
    "                        img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "\n",
    "                if False:\n",
    "                    all_samples.append(x_checked_image_torch)\n",
    "\n",
    "                if False:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                    # to image\n",
    "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                    img = Image.fromarray(grid.astype(np.uint8))\n",
    "                    img = put_watermark(img, wm_encoder)\n",
    "                    img.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "                    grid_count += 1\n",
    "\n",
    "                return samples_ddim\n",
    "\n",
    "def degration(image):\n",
    "    # transform the image to gray-scale\n",
    "    r = image[:, 0, :, :]\n",
    "    g = image[:, 1, :, :]\n",
    "    b = image[:, 2, :, :]\n",
    "    n, _, h, w = image.size()\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    image = gray.view(n, 1, h, w).expand(n, 3, h, w)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1451a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUDA:0 (NVIDIA A100 80GB PCIe, 80995MiB)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b8034a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/v-penxiao/workspace/DailyCkpts/stable_diffusion/sd-v1-4-full-ema.ckpt \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/sd-v1-4-full-ema.ckpt '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[39m=\u001b[39m OmegaConf\u001b[39m.\u001b[39mload(config_path)\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m load_model_from_config(config, diffusion_model_path)\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m vgg \u001b[39m=\u001b[39m VGGPerceptualLoss()\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, ckpt, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model_from_config\u001b[39m(config, ckpt, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading model from \u001b[39m\u001b[39m{\u001b[39;00mckpt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     pl_sd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(ckpt, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m pl_sd:\n\u001b[1;32m     30\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGlobal Step: \u001b[39m\u001b[39m{\u001b[39;00mpl_sd[\u001b[39m'\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    232\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/anaconda/envs/ldm/lib/python3.8/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/v-penxiao/workspace/DailyCkpts/stable_diffusion/sd-v1-4-full-ema.ckpt '"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(config_path)\n",
    "model = load_model_from_config(config, diffusion_model_path)\n",
    "model = model.to(device)\n",
    "vgg = VGGPerceptualLoss().to(model.device)\n",
    "\n",
    "if False:\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "start_code = None\n",
    "\n",
    "if False:\n",
    "    start_codenamed_parametersrandn([n_samples, 4, H // 8, W // 8], device=device)\n",
    "    \n",
    "precision_scope = autocast if True else nullcontext\n",
    "batch_size = 1\n",
    "n_rows = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad527f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of a yellow dog sitting on a black bench, background is blue sky and green grass land.\"\n",
    "# prompt = \"a colorful photograph of a black horse standing on a path in the countryside with a clear sky in the background.\"\n",
    "src_image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "src_image = src_image.astype(np.float32) / 255.0\n",
    "src_image = src_image[None].transpose(0, 3, 1, 2)\n",
    "src_image = torch.from_numpy(src_image)\n",
    "src_image = src_image * 2.0 - 1.0\n",
    "src_image = src_image.to(device)\n",
    "\n",
    "un_cond = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "cond = model.get_learned_conditioning([prompt])\n",
    "src_image_latent = model.encode_first_stage(src_image)\n",
    "x_start = model.get_first_stage_encoding(src_image_latent)\n",
    "\n",
    "cond.requires_grad = True\n",
    "cond_copy = cond.clone()\n",
    "x_start.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:03<00:00, 14.65it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    _ = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, outpath=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38859887",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- embedding optimization --------------------\n",
      "curr_t= 0 ; loss= 0.07409411668777466\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 99 ; loss= 0.20533126592636108\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 199 ; loss= 0.1895008385181427\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 299 ; loss= 0.06506029516458511\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 399 ; loss= 0.26059526205062866\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 499 ; loss= 0.4232313632965088\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving optimized_cond to path: /root/stable/stable-diffusion/outputs/imagic/opti_cond_xloss.pt\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    #1. embedding optimization\n",
    "'''\n",
    "print(\"-\" * 20, \"embedding optimization\", \"-\" * 20)\n",
    "optimizer = torch.optim.Adam([cond], lr=0.001)\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "steps = 500\n",
    "for t_int in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model(not available)'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "    \n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    loss = criteria(pred_noise, noise)\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer.step()\n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, outpath=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_path)\n",
    "torch.save(cond, cond_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd9f57dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- finetune diffusion model --------------------\n",
      "cond.requires_grad= False\n",
      "curr_t= 0 ; loss= 0.22337594628334045\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 99 ; loss= 0.19026441872119904\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 199 ; loss= 0.09106290340423584\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 299 ; loss= 0.2246825397014618\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 399 ; loss= 0.22303630411624908\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 499 ; loss= 0.15712252259254456\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 599 ; loss= 0.11131340265274048\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 699 ; loss= 0.06759205460548401\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 799 ; loss= 0.04386652261018753\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 899 ; loss= 0.04322316125035286\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 999 ; loss= 0.024342142045497894\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 1099 ; loss= 0.022764425724744797\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 1199 ; loss= 0.02561568096280098\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 1299 ; loss= 0.012350468896329403\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 1399 ; loss= 0.009830348193645477\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 1499 ; loss= 0.010640243999660015\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving finetuned unet-model to path: /root/stable/stable-diffusion/outputs/imagic/opti_unet_xloss.pt\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    #2. finetune diffusion model\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune diffusion model\", \"-\" * 20)\n",
    "cond = torch.load(cond_path)\n",
    "cond.requires_grad = False\n",
    "print(\"cond.requires_grad=\",cond.requires_grad)\n",
    "model.model.train()\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=1e-6)\n",
    "steps = 1500\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(x_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(x_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, cond)\n",
    "    # pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "      \n",
    "    # pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "    \n",
    "    # loss = 0.0 * criteria(pred_x0_gray_latent, x_start)  + 0. * criteria(pred_noise, noise) + 0. * vgg(pred_x0_gray, src_image)\n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    # loss = criteria(pred_x0, x_start)\n",
    "    loss = criteria(pred_noise, noise)\n",
    "\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    \n",
    "    if (t_int+1) % 100 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        with torch.no_grad():\n",
    "            _ = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, outpath=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "print(\"saving finetuned unet-model to path:\", unet_path)\n",
    "torch.save(model.model.state_dict(), unet_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995c14a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.37it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    _ = generate_by_prompt(wm_encoder=wm_encoder, c=cond, samplER=sampler, model=model, outpath=output_path, start_code=None,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b02358-80f8-4353-a6d4-46e19b602a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond = torch.load(cond_path)\n",
    "model.model.load_state_dict(torch.load(unet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb88846d-af37-4a5a-9a66-3f9a4360691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn_like(x_start)\n",
    "# t_tensor =torch.Tensor([1.]).long().to(device)\n",
    "# x_noisy = model.q_sample(new_start, t_tensor, noise=noise)\n",
    "# x_noisy_image = torch.clamp((model.differentiable_decode_first_stage(x_noisy) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "\n",
    "# '''for eps prediction model'''\n",
    "# alpha = 1.7\n",
    "# new_cond = cond * torch.Tensor([1-alpha]).to(device) + model.get_learned_conditioning([prompt]) * torch.Tensor([alpha]).to(device)\n",
    "# pred_noise = model.apply_model(x_noisy, t_tensor, new_cond)\n",
    "# pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "# pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "\n",
    "# save_image(x_noisy_image, \"yyy.jpg\")\n",
    "# save_image(pred_x0_rgb, \"xxx.jpg\")\n",
    "# # with torch.no_grad():\n",
    "# #     _ = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, outpath=output_path, start_code=pred_x0, h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dfe5608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:04<00:00, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "model.model.eval()\n",
    "cond.requires_grad = False\n",
    "alpha = 1.7\n",
    "new_c = cond * torch.Tensor([1-alpha]).to(device) + cond_copy * torch.Tensor([alpha]).to(device)\n",
    "# new_c = cond * torch.Tensor([1-alpha]).to(device) + model.get_learned_conditioning([\"a colorful photograph of a white horse standing on a path in the countryside with a clear sky in the background.\"]) * torch.Tensor([alpha]).to(device)\n",
    "# new_c =  cond_copy * torch.Tensor([2]).to(device)\n",
    "# new_c = cond * torch.Tensor([1-alpha]).to(device) + torch.Tensor([alpha]).to(device) * model.get_learned_conditioning([\"a photograph of a yellow dog sitting on a black bench, background is green grassland, some trees and a blue sky\"])\n",
    "# new_c = cond * torch.Tensor([1-alpha]).to(device) + torch.Tensor([alpha]).to(device) * model.get_learned_conditioning([\"a photograph of a yellow shiba inu sitting on a big sofa\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = generate_by_prompt(wm_encoder=wm_encoder, c=new_c, samplER=sampler, model=model, outpath=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0471cfb-7fa6-4721-a208-117e337588ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_c.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d18480e-1e70-40e5-b8c9-3200503aff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cond = cond * torch.Tensor([1-alpha]).to(device) + model.get_learned_conditioning([prompt]) * torch.Tensor([alpha]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "644511c6-e840-4598-b09f-c7d049219892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:04<00:00, 10.91it/s]\n"
     ]
    }
   ],
   "source": [
    "new_start = generate_by_prompt(wm_encoder=wm_encoder, c=new_cond, samplER=sampler, model=model, outpath=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=1, scale=3, ddim_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b2ceb9-965c-458f-a6cc-713771edd805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- finetune for reconstruction --------------------\n",
      "curr_t= 0 ; loss= 5.040149688720703\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 49 ; loss= 5.187790870666504\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 99 ; loss= 5.03861665725708\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 149 ; loss= 4.9116597175598145\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 199 ; loss= 4.922728061676025\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 249 ; loss= 4.8289899826049805\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 299 ; loss= 4.809016227722168\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 349 ; loss= 4.798003673553467\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 399 ; loss= 4.708376884460449\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 449 ; loss= 4.69287633895874\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 499 ; loss= 4.947040557861328\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 549 ; loss= 4.696888446807861\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 599 ; loss= 5.23191499710083\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_t= 649 ; loss= 4.811708450317383\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 46 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 46/46 [00:04<00:00, 10.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# pred_x0_gray = degration(pred_x0_rgb)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# loss = criteria(pred_x0_gray_latent, x_start)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m vgg(pred_x0_rgb, src_image)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     41\u001b[0m optimizer_finetune\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    #3. finetune diffusion model for reconstruction\n",
    "'''\n",
    "print(\"-\" * 20, \"finetune for reconstruction\", \"-\" * 20)\n",
    "model.model.train()\n",
    "# optimizer_finetune = torch.optim.Adam([\n",
    "#                                         {'params':[new_cond], 'lr':1e-3},\n",
    "#                                         {'params':model.model.parameters(), 'lr':1e-7}], \n",
    "#                                         lr=1e-6)\n",
    "# optimizer_finetune = torch.optim.Adam([new_cond], lr=0.001)\n",
    "optimizer_finetune = torch.optim.Adam(model.model.parameters(), lr=1e-8)\n",
    "new_cond.requires_grad = False\n",
    "steps = 1000\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "for t_int in range(steps):\n",
    "    optimizer_finetune.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(new_start)\n",
    "    t_tensor = torch.randint(0, model.num_timesteps, (new_start.shape[0],), device=model.device).long()\n",
    "    x_noisy = model.q_sample(new_start, t_tensor, noise=noise)\n",
    "    \n",
    "    '''for eps prediction model'''\n",
    "    pred_noise = model.apply_model(x_noisy, t_tensor, new_cond)\n",
    "    pred_x0 = model.predict_start_from_noise(x_noisy, t_tensor, pred_noise)\n",
    "    '''for x0 prediction model'''\n",
    "    # pred_x0 = model.apply_model(x_noisy, t_tensor, cond)\n",
    "      \n",
    "    pred_x0_rgb = torch.clamp((model.differentiable_decode_first_stage(pred_x0) + 1.0) / 2.0, min=-1.0, max=1.0)\n",
    "    # pred_x0_gray = degration(pred_x0_rgb)\n",
    "    # pred_x0_gray_latent = model.get_first_stage_encoding(model.differentiable_encode_first_stage(pred_x0_gray))\n",
    "\n",
    "    # save_image(pred_x0_rgb,\"pred_rgb.jpg\")\n",
    "    # save_image(pred_x0_gray,\"pred_gray.jpg\")\n",
    "    \n",
    "    # loss = criteria(pred_x0_gray_latent, x_start)\n",
    "    loss = vgg(pred_x0_rgb, src_image)\n",
    "    loss.backward()\n",
    "    history.append(loss.item())\n",
    "    optimizer_finetune.step()\n",
    "    \n",
    "    if (t_int+1) % 50 ==0 or t_int ==0:\n",
    "        print(\"curr_t=\", t_int, \"; loss=\", loss.item())\n",
    "        # new_cond = new_cond * torch.Tensor([1-0.1]).to(device) + model.get_learned_conditioning([prompt]) * torch.Tensor([0.1]).to(device)\n",
    "        _ = generate_by_prompt(wm_encoder=wm_encoder, c=new_cond, samplER=sampler, model=model, outpath=output_path, start_code=start_code,h=512, w=512, ddim_eta=0.0, n_samples=batch_size, scale=3, ddim_steps=45)\n",
    "\n",
    "# unet_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_unet.pt\"\n",
    "\n",
    "# print(\"saving finetuned unet-model to path:\", unet_for_pixel_path)\n",
    "# torch.save(model.model.state_dict(), unet_for_pixel_path)\n",
    "# print(\"Done\\n\")\n",
    "\n",
    "cond_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_cond.pt\"\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "085b0227-d4c2-4e44-b995-703c6a8c0a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12c999e8-ee5b-4bf4-b6c4-1cfa2df1a442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1969, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria(pred_x0_gray, src_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82baf32e-1127-4f11-bb08-14517a9761a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving optimized_cond to path: /root/stable/stable-diffusion/outputs/imagic/pixel_cond_horse.pt\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cond_for_pixel_path = \"/root/stable/stable-diffusion/outputs/imagic/pixel_cond_horse.pt\"\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_for_pixel_path)\n",
    "torch.save(new_cond, cond_for_pixel_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e76c8db-927d-414b-9e25-f8db067a7f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8935, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''more stage 2 '''\n",
    "vgg(pred_x0_gray, src_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21cf8527-2f19-4e58-8f85-b444d58201c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4580, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2. * criteria(pred_x0_gray, src_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbdbe23-0f2c-41be-871a-ed0cb69ca8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3d11d",
   "metadata": {},
   "source": [
    "for name, para in model.named_parameters():\n",
    "    print(name, para.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_path = \"/home/xiaopeng/stable-diffusion/outputs/11_19_500_cond.pt\"\n",
    "unet_path = \"/home/xiaopeng/stable-diffusion/outputs/11_19_600_unet.pt\"\n",
    "\n",
    "print(\"saving optimized_cond to path:\", cond_path)\n",
    "torch.save(cond, cond_path)\n",
    "print(\"Done\\n\")\n",
    "\n",
    "print(\"saving finetuned unet-model to path:\", unet_path)\n",
    "torch.save(model.model.state_dict(), unet_path)\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from ignite.engine import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.utils import *\n",
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "\n",
    "# create default evaluator for doctests\n",
    "\n",
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "# create default optimizer for doctests\n",
    "\n",
    "param_tensor = torch.zeros([1], requires_grad=True)\n",
    "default_optimizer = torch.optim.SGD([param_tensor], lr=0.1)\n",
    "\n",
    "# create default trainer for doctests\n",
    "# as handlers could be attached to the trainer,\n",
    "# each test must define his own trainer using `.. testsetup:`\n",
    "\n",
    "def get_default_trainer():\n",
    "\n",
    "    def train_step(engine, batch):\n",
    "        return batch\n",
    "\n",
    "    return Engine(train_step)\n",
    "\n",
    "# create default model for doctests\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "manual_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db077fea-f0d4-46a2-ad06-f1dcf6bb4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = PSNR(data_range=1.0)\n",
    "psnr.attach(default_evaluator, 'psnr')\n",
    "preds = torch.rand([4, 3, 16, 16])\n",
    "target = torch.rand([4, 3, 16, 16])\n",
    "state = default_evaluator.run([[preds, target]])\n",
    "print(state.metrics['psnr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ab02e-12e2-4582-a439-269562ade987",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = SSIM(data_range=1.0)\n",
    "metric.attach(default_evaluator, 'ssim')\n",
    "preds = torch.rand([4, 3, 16, 16])\n",
    "target = torch.rand([4, 3, 16, 16])\n",
    "state = default_evaluator.run([[preds, target]])\n",
    "print(state.metrics['ssim'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2284b64b7a38d4e06dbb5f84cc3277933c03c9b00bcd12a39d3717fb0a62a2d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
